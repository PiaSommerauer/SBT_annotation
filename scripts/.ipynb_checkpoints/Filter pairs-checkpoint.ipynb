{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exlcude confusing pairs prior to annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Prepare annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import os \n",
    "from collections import defaultdict\n",
    "\n",
    "from utils import read_group, read_csv, to_csv\n",
    "\n",
    "def load_pairs(exp_group):\n",
    "    \n",
    "    all_pairs = []\n",
    "    collections = ['perceptual', 'activities', 'complex', 'parts']\n",
    "    prop_coll_dict = read_group(exp_group)\n",
    "    \n",
    "    for collection in collections:\n",
    "        f = f'../data/{collection}.csv'\n",
    "        dict_list = read_csv(f)\n",
    "        for d in dict_list:\n",
    "            prop = d['property']\n",
    "            if prop in prop_coll_dict:\n",
    "                all_pairs.append(d)\n",
    "    return all_pairs\n",
    "\n",
    "def shrink_dict(d, keys):\n",
    "    current_keys = list(d.keys())\n",
    "    for k in current_keys:\n",
    "        if k not in keys:\n",
    "            d.pop(k)\n",
    "    return d\n",
    "            \n",
    "\n",
    "def to_file(all_pairs, exp_group):\n",
    "    header = ['property', 'lemma']\n",
    "    all_pairs_clean = [shrink_dict(d, header) for d in all_pairs]\n",
    "    f = f'../pair_filtering/{exp_group}.csv'\n",
    "    to_csv(f, all_pairs_clean)\n",
    "    \n",
    "    \n",
    "def divide_to_file(exp_group, annotators):\n",
    "    all_pairs = load_pairs(exp_group)\n",
    " \n",
    "    \n",
    "    exp_dir = f'../data_pair_filtering/to_annotate/{exp_group}/'\n",
    "    if not os.path.isdir(exp_dir):\n",
    "        os.mkdir(exp_dir)\n",
    "    \n",
    "    prop_dict = defaultdict(list)\n",
    "    for d in all_pairs:\n",
    "        header = ['property', 'lemma']\n",
    "        d_clean = shrink_dict(d, header) \n",
    "        prop = d_clean['property']\n",
    "        prop_dict[prop].append(d_clean)\n",
    "\n",
    "    for prop, dict_list in prop_dict.items():\n",
    "        for annotator in annotators:\n",
    "            f = f'{exp_dir}/{prop}-{annotator}.csv'\n",
    "            to_csv(f, dict_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate annotation sheets - already done \n",
    "#exp_group = 'experiment3'   \n",
    "#annotators = ['pia', 'antske']\n",
    "#divide_to_file(exp_group, annotators)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Annotate via Google sheets\n",
    "\n",
    "Annotation instructions, links and status overview are [here](https://docs.google.com/document/d/1tupla_Jhr1hXt0CEle0Rji355zZpBZeGXugPMcKRjLM/edit?usp=sharing). \n",
    "\n",
    "Once annotationes are down, download the sheets as .csv files and store in `../pair_filtering/annotated/exp_name/property.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Analyze agreement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import read_group, read_csv, to_csv\n",
    "\n",
    "\n",
    "def load_annotations_file(f):\n",
    "    print(f)\n",
    "    dict_list = read_csv(f)\n",
    "    return dict_list\n",
    "\n",
    "\n",
    "def analyze_annotations(exp_group, prop, name1, name2):\n",
    "    \n",
    "    dir_annotated = f'../data_pair_filtering/annotated/{exp_group}'\n",
    "  \n",
    "    f = f'{dir_annotated}/{prop}-{name1}.csv'\n",
    "    dict_list_1 = load_annotations_file(f)\n",
    "    f = f'{dir_annotated}/{prop}-{name2}.csv'\n",
    "    dict_list_2 = load_annotations_file(f)\n",
    "    data_annotated = []\n",
    "    \n",
    "    for d_1, d_2 in zip(dict_list_1, dict_list_2):\n",
    "        new_d = dict()\n",
    "        concept = d_1['lemma']\n",
    "        if concept != d_2['lemma']:\n",
    "            print('problem: data do not match')\n",
    "        l_1 = d_1['decision']\n",
    "        l_2 = d_2['decision']\n",
    "        new_d['lemma'] = concept\n",
    "        new_d[name1] = l_1\n",
    "        new_d[name2] = l_2\n",
    "        #print(concept, l_1, l_2)\n",
    "        if l_1 != l_2:\n",
    "            decision = 'exclude1'\n",
    "            #contradictions.append(new_d)\n",
    "        elif l_1 == l_2 == 'exclude':\n",
    "            decision = 'exclude2'\n",
    "        elif l_1 == l_2 == 'include':\n",
    "            decision = 'include'\n",
    "        new_d['decision'] = decision\n",
    "        data_annotated.append(new_d)\n",
    "            \n",
    "    include = [d for d in data_annotated if d['decision'] == 'include'] \n",
    "    exclude1 = [d for d in data_annotated if d['decision'] == 'exclude1']\n",
    "    exclude_agree = [d for d in data_annotated if d['decision'] == 'exclude2'] \n",
    "    print(f'Total number of concepts: {len(data_annotated)}')\n",
    "    print(f'Total number included by both: {len(include)}')\n",
    "    print(f'Total number excluded by one person: {len(exclude1)}')\n",
    "    print(f'Total number of agreements on exlcude: {len(exclude_agree)}')\n",
    "    if exclude_agree:\n",
    "        print('Agreed on excluding:')\n",
    "        for d in exclude_agree:\n",
    "            print(d['lemma'])\n",
    "    return data_annotated\n",
    "\n",
    "\n",
    "def aggregated_to_file(prop, data_annotated, exp_group):\n",
    "    \n",
    "    dir_path = f'../data_pair_filtering/aggregated/{exp_group}'\n",
    "    if not os.path.isdir(dir_path):\n",
    "        os.mkdir(dir_path)\n",
    "    \n",
    "    file_path = f'{dir_path}/{prop}.csv'\n",
    "    to_csv(file_path, data_annotated)\n",
    "    \n",
    "    \n",
    "def replace_concept(data_annotated, include, replace):\n",
    "    # set for juicy only: find variants of potato and exclude. leave in potato\n",
    "\n",
    "    potato_include = 'potato'\n",
    "    potato_variants = ['tater', 'spud']\n",
    "    all_concepts = [d['lemma'] for d in data_annotated]\n",
    "    if potato_include in all_concepts:\n",
    "        print('found', potato_include)\n",
    "    for d in data_annotated:\n",
    "        if d['lemma'] in potato_variants:\n",
    "            d['decision'] = 'exclude'\n",
    "            print(d)\n",
    "        elif d['lemma'] == potato_include:\n",
    "            d['decision'] = 'include'\n",
    "            print(d)\n",
    "            \n",
    "            \n",
    "def add_concepts(data_annotated, concepts_to_include, name1, name2):\n",
    "    all_concepts_to_include = set([d['lemma'] for d in concepts_to_include])\n",
    "    for c in concepts_to_include:\n",
    "        if c not in all_concepts_to_include:\n",
    "            d = dict()\n",
    "            d['lemma'] = c\n",
    "            d[name1] = 'add'\n",
    "            d[name2] = 'add'\n",
    "            d['decision'] = 'added'\n",
    "            data_annotated.append(d)\n",
    "        else:\n",
    "            print('Already in included concepts': c)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregation and replacements\n",
    "\n",
    "### juicy (special case)\n",
    "\n",
    "- replaced different variants of potato: potato should stay, tater and spud should be excluded\n",
    "\n",
    "### green\n",
    "\n",
    "- aggregated and stored\n",
    "\n",
    "### swim\n",
    "\n",
    "- aggregated and stored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data_pair_filtering/annotated/experiment3/green-antske.csv\n",
      "../data_pair_filtering/annotated/experiment3/green-pia.csv\n",
      "Total number of concepts: 175\n",
      "Total number included by both: 158\n",
      "Total number excluded by one person: 10\n",
      "Total number of agreements on exlcude: 7\n",
      "Agreed on excluding:\n",
      "plumeria\n",
      "lewisia\n",
      "palaquium\n",
      "manilkara\n",
      "platanthera\n",
      "colocasia\n",
      "teucrium\n"
     ]
    }
   ],
   "source": [
    "# Code\n",
    "\n",
    "exp_group = 'experiment3'\n",
    "name1 = 'antske'\n",
    "name2 = 'pia'\n",
    "prop = 'green'\n",
    "\n",
    "concepts_to_include = []\n",
    "\n",
    "data_annotated = analyze_annotations(exp_group, prop, name1, name2)\n",
    "\n",
    "#add_concepts(data_annotated, concepts_to_include, name1, name2)\n",
    "aggregated_to_file(prop, data_annotated, exp_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cold\n",
    "-  discuss before aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data_pair_filtering/annotated/experiment3/cold-antske.csv\n",
      "../data_pair_filtering/annotated/experiment3/cold-pia.csv\n",
      "Total number of concepts: 121\n",
      "Total number included by both: 105\n",
      "Total number excluded by one person: 12\n",
      "Total number of agreements on exlcude: 4\n",
      "Agreed on excluding:\n",
      "iliamna\n",
      "mentzelia\n",
      "crack\n",
      "perithecia\n"
     ]
    }
   ],
   "source": [
    "# code cold\n",
    "\n",
    "# cold\n",
    "exp_group = 'experiment3'\n",
    "name1 = 'antske'\n",
    "name2 = 'pia'\n",
    "prop = 'cold'\n",
    "\n",
    "concepts_to_include = []\n",
    "\n",
    "data_annotated = analyze_annotations(exp_group, prop, name1, name2)\n",
    "\n",
    "#add_concepts(data_annotated, concepts_to_include, name1, name2)\n",
    "\n",
    "#aggregated_to_file(prop, data_annotated, exp_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wings\n",
    "\n",
    "\n",
    "- Comment Antske: Done:\n",
    "I think there are too many cars or car-like things in the corpus and more `borderline’ or `odd’ vehicles would be nice.\n",
    "I also think we want to include Penguin and Platypus, possibly other animals. Insects are probably more interesting than birds.\n",
    "\n",
    "* agggregate\n",
    "* add words \n",
    "* make sure these words can be used by the sampling algorith\n",
    "* Suggestion for added words below - we can add more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "kite helicopter airplane zeppelin balloon parachute hovercraft penguin ostrich chicken emu kiwi platypus bee dragonfly ant ladybird butterfly\n",
      "\n",
      "../data_pair_filtering/annotated/experiment3/wings-antske.csv\n",
      "../data_pair_filtering/annotated/experiment3/wings-pia.csv\n",
      "Total number of concepts: 147\n",
      "Total number included by both: 101\n",
      "Total number excluded by one person: 27\n",
      "Total number of agreements on exlcude: 19\n",
      "Agreed on excluding:\n",
      "procellariidae\n",
      "strigidae\n",
      "cotingidae\n",
      "timaliidae\n",
      "alaudidae\n",
      "apodidae\n",
      "laridae\n",
      "fringillidae\n",
      "muscicapa\n",
      "caprimulgus\n",
      "phalacrocorax\n",
      "puffinus\n",
      "calidris\n",
      "motacilla\n",
      "emberizidae\n",
      "haliaeetus\n",
      "charadrius\n",
      "paridae\n",
      "icteridae\n"
     ]
    }
   ],
   "source": [
    "# Wings\n",
    "exp_group = 'experiment3'\n",
    "name1 = 'antske'\n",
    "name2 = 'pia'\n",
    "prop = 'wings'\n",
    "\n",
    "# check car brands - possibly exclude\n",
    "# different words for airplane (possibly add after annotation)\n",
    "concepts_to_include = ['kite', 'helicopter', 'airplane', 'zeppelin', \n",
    "                       'balloon', 'parachute', 'hovercraft',\n",
    "                   'penguin', 'ostrich', 'chicken', 'emu', 'kiwi',\n",
    "                   'platypus', \n",
    "                   'bee', 'dragonfly', 'ant', 'ladybird', 'butterfly']\n",
    "print()\n",
    "print(' '.join(concepts_to_include))\n",
    "print()\n",
    "data_annotated = analyze_annotations(exp_group, prop, name1, name2)\n",
    "\n",
    "add_concepts(data_annotated, concepts_to_include, name1, name2)\n",
    "\n",
    "aggregated_to_file(prop, data_annotated, exp_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get lexical data for additional concepts\n",
    "\n",
    "(1) add concepts to feature_data/data/manually_included_after_centroid/properties_selected_run5.csv\n",
    "\n",
    "(2) In feature_data: cd scripts_process_raw_data, run add_preselected_concepts_after_centroid_step.py [collection]\n",
    "\n",
    "(3) copy feature_data/data/concepts_additional_info_manual_run5_pilot to SPT_annotation/data_all_candidates/. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge new sets with already created question sets\n",
    "\n",
    "In SPT_annotation: \n",
    "\n",
    "(1) run replace_excluded_concepts.py [property] [collection] [run] \n",
    "\n",
    "(2) run update_dataset.py [run] [prop] [run_new_label]  (use new label if you want to keep runs separate\n",
    "\n",
    "(3) run create_questions.py [run_new_label] (you will have to copy the template file from the previous run and use the new run label)\n",
    "\n",
    "(4) Merge old question file with new question file (make backups of both before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the question files\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_original = '../questions/run5_pilot-all-restricted_True.csv'\n",
    "file_new = '../questions/run5_part2-all-restricted_True.csv'\n",
    "\n",
    "# write original data to a new file:\n",
    "file_original_backup = '../questions/run5_part1-all-restricted_True.csv'\n",
    "with open(file_original) as infile:\n",
    "    original = infile.read()\n",
    "    \n",
    "with open(file_original_backup, 'w') as outfile:\n",
    "    outfile.write(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6666 5138\n",
      "11804\n"
     ]
    }
   ],
   "source": [
    "# load original data as dict_list\n",
    "\n",
    "with open(file_original) as infile:\n",
    "    dict_list_original = list(csv.DictReader(infile, delimiter = '\\t'))\n",
    "    \n",
    "# load new file\n",
    "\n",
    "with open(file_new) as infile:\n",
    "    dict_list_new = list(csv.DictReader(infile, delimiter = '\\t'))\n",
    "    \n",
    "print(len(dict_list_original), len(dict_list_new))\n",
    "\n",
    "# add new to original\n",
    "dict_list_original.extend(dict_list_new)\n",
    "print(len(dict_list_original))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extended file to originl path\n",
    "\n",
    "header = dict_list_original[0].keys()\n",
    "with open(file_original, 'w') as outfile:\n",
    "    writer = csv.DictWriter(outfile, fieldnames = header, delimiter = '\\t')\n",
    "    writer.writeheader()\n",
    "    for d in dict_list_original:\n",
    "        writer.writerow(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
