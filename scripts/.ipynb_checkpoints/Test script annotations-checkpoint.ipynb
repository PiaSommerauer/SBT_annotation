{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests\n",
    "\n",
    "* check if there are duplicate questions\n",
    "* check if there are duplicates across runs and experiments\n",
    "* check if there are duplicates within runs and experiments\n",
    "* check if there are duplicates within batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import glob\n",
    "#from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## restructuring batch code\n",
    "\n",
    "from create_multibatches import read_input, get_questions, get_batch\n",
    "from utils import read_csv, sort_by_key\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "available for batch:\n",
      "not annotated yet: 5412\n",
      "not valid 0\n",
      "availabel in total:\n",
      "not annotated yet: 5412\n",
      "not valid 0\n",
      "Percentage of annotated questions of experiment3: 18.8%\n",
      "Number of questions in the total dataset: 6666\n",
      "Number of questions in experiment3: 6666\n",
      "Number of annotated questions: 1254        (of which in experiment3: 1254)\n",
      "Percentage of annotated questions of the total: 18.8%\n",
      "current batch number: 8\n",
      "210\n",
      "still more than 210 questions available.\n",
      "found enough questions 210\n",
      "210\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "run = '5_pilot'\n",
    "experiment_name = 'experiment3'\n",
    "n_qu = 70\n",
    "n_lists = 3\n",
    "test = False\n",
    "\n",
    "all_input_dicts, input_dicts_batch, batch_numbers = read_input(run, experiment_name)\n",
    "\n",
    "questions_in_selection = get_questions(run, experiment_name, all_input_dicts, input_dicts_batch)\n",
    "test_question_path = f'../questions/run{run}-TEST.csv'\n",
    "test_question_dicts = read_csv(test_question_path)\n",
    "\n",
    "# Create new batch\n",
    "if batch_numbers:\n",
    "    highest_batch_number = max(batch_numbers)\n",
    "else:\n",
    "    highest_batch_number = 0\n",
    "if test == False:\n",
    "    current_batch_n = highest_batch_number + 1\n",
    "else:\n",
    "    current_batch_n = 'TEST'\n",
    "\n",
    "print('current batch number:', current_batch_n)\n",
    "full_batch = []\n",
    "# collect all questions in one go:\n",
    "n_qu_total = n_qu * n_lists\n",
    "print(n_qu_total)\n",
    "new_batch = get_batch(questions_in_selection, n_qu=n_qu_total)\n",
    "print(len(new_batch))\n",
    "\n",
    "\n",
    "print(len(new_batch_by_pair))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_over_lists(new_batch):\n",
    "    \n",
    "    batch_with_listnumbers = []\n",
    "    \n",
    "    new_batch_by_pair = sort_by_key(new_batch, ['property', 'concept'])\n",
    "    pair_by_n = defaultdict(list)\n",
    "    for p, data in new_batch_by_pair.items():\n",
    "        pair_by_n[len(data)].append(p)\n",
    "\n",
    "    pairs_assigned = set()\n",
    "    batch_dict = defaultdict(list)\n",
    "    for n, pairs in pair_by_n.items():\n",
    "        for list_n in range(n_lists):\n",
    "            list_n = list_n+1\n",
    "            for p in pairs:\n",
    "                if p not in pairs_assigned:\n",
    "                    questions = new_batch_by_pair[p]\n",
    "                    batch_dict[list_n].extend(questions)\n",
    "                    pairs_assigned.add(p)\n",
    "                    if len(batch_dict[list_n]) >= n_qu:\n",
    "                        break\n",
    "                        \n",
    "    \n",
    "    for list_n, questions in batch_dict.items():\n",
    "        print(list_n, len(questions))\n",
    "        for d in questions:\n",
    "            d['listnr'] = list_n\n",
    "            batch_with_listnumbers.append(d)\n",
    "    return batch_with_listnumbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 70\n",
      "2 70\n",
      "3 70\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "{'property': 'blue', 'label': 'neg', 'certainty': 'not_certain', 'concept': 'buzzard', 'collection': 'perceptual', 'sources': 'mcrae', 'quid': 'ace9ede6-e23a-4b25-b655-f909d291c6b3', 'relation': 'variability_open', 'question': 'You can find (a/an) buzzard which is blue. Blue is one of many possible colors (a/an) buzzard usually has. The range of colors is almost unlimited. ', 'prop_pos': 'purple', 'concept_pos': 'flower', 'prop_neg': 'orange', 'concept_neg': 'carrot', 'example_pos': 'You can find (a/an) flower which is purple. Purple is one of many possible colors (a/an) flower usually has. The range of colors is almost unlimited. ', 'example_neg': 'You can find (a/an) carrot which is orange. Orange is one of many possible colors (a/an) carrot usually has. The range of colors is almost unlimited. ', 'listnr': 3}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 Agree or disagree (run5_pilot-experiment3-batch2-70-70)\n",
      "222 Agree or disagree (run5_pilot-experiment3-batch7-70-70)\n",
      "216 Agree or disagree (run5_pilot-experiment3-batch6-70-70)\n",
      "48 Agree or disagree (run5_pilot-experiment3-batch1-20-20)\n",
      "226 Agree or disagree (run5_pilot-experiment3-batch4-70-70)\n",
      "224 Agree or disagree (run5_pilot-experiment3-batch5-70-70)\n",
      "222 Agree or disagree (run5_pilot-experiment3-batch3-70-70)\n",
      "\n",
      "creative-warm-cincture {'Agree or disagree (run5_pilot-experiment3-batch4-70-70)'}\n",
      "impossible-warm-cincture {'Agree or disagree (run5_pilot-experiment3-batch4-70-70)'}\n",
      "typical_of_property-warm-cincture {'Agree or disagree (run5_pilot-experiment3-batch4-70-70)'}\n",
      "implied_category-warm-cincture {'Agree or disagree (run5_pilot-experiment3-batch4-70-70)'}\n",
      "variability_open-warm-cincture {'Agree or disagree (run5_pilot-experiment3-batch4-70-70)'}\n",
      "rare-warm-cincture {'Agree or disagree (run5_pilot-experiment3-batch4-70-70)'}\n",
      "variability_limited-warm-cincture {'Agree or disagree (run5_pilot-experiment3-batch4-70-70)'}\n",
      "unusual-warm-cincture {'Agree or disagree (run5_pilot-experiment3-batch4-70-70)'}\n",
      "typical_of_concept-warm-cincture {'Agree or disagree (run5_pilot-experiment3-batch4-70-70)'}\n",
      "affording_activity-warm-cincture {'Agree or disagree (run5_pilot-experiment3-batch4-70-70)'}\n"
     ]
    }
   ],
   "source": [
    "run = '5_pilot'\n",
    "exp = '*'\n",
    "batch = '*'\n",
    "path = f'../prolific_input/run{run}-group_experiment{exp}/qu*-s_qu*-batch{batch}.csv'\n",
    "\n",
    "all_triples_dict = defaultdict(list)\n",
    "\n",
    "for f in glob.glob(path):\n",
    "    data_dicts = read_csv(f)\n",
    "    print(len(data_dicts), data_dicts[0]['name'])\n",
    "    data_by_triple = sort_by_key(data_dicts, ['triple'])\n",
    "    all_triples_dict.update(data_by_triple)\n",
    "print()\n",
    "    \n",
    "for triple, data in all_triples_dict.items():\n",
    "    if len(data) > 1:\n",
    "        # check if test:\n",
    "        quids = [d['quid'] for d in data]\n",
    "        test_check = [quid.startswith('test') for quid in quids]\n",
    "        if all(test_check):\n",
    "            continue\n",
    "        else:\n",
    "            batches = set([d['name'] for d in data])\n",
    "            print(triple, batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revise batch code with checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1086\n"
     ]
    }
   ],
   "source": [
    "def get_data_run_exp(run, experiment_name):\n",
    "    input_dicts_run_exp = []\n",
    "    f_paths_run_name = glob.glob(f'../prolific_input/run{run}-group_{experiment_name}/*.csv')\n",
    "    header_path = f'../prolific_input/run{run}-group_{experiment_name}/header.txt'\n",
    "    with open(header_path) as infile:\n",
    "        header = infile.read().split(',')\n",
    "    for f in f_paths_run_name:\n",
    "        input_dicts = read_csv(f, header = header)\n",
    "        input_dicts_run_exp.extend(input_dicts)\n",
    "    return input_dicts_run_exp\n",
    "            \n",
    "run = '5_pilot'\n",
    "experiment_name = 'experiment3' \n",
    "data_annotated = get_data_run_exp(run, experiment_name)\n",
    "print(len(data_annotated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19458\n",
      "1086\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_input(run, experiment_name):\n",
    "    all_input_dicts = []\n",
    "    input_dicts_run_exp = []\n",
    "    # all files\n",
    "    f_paths_all = glob.glob(f'../prolific_input/run*-group_*/*.csv')\n",
    "    # files run and experiment\n",
    "    f_paths_run_name = glob.glob(f'../prolific_input/run{run}-group_{experiment_name}/*.csv')\n",
    "    header_path = f'../prolific_input/run{run}-group_{experiment_name}/header.txt'\n",
    "    batch_numbers = []\n",
    "    # Get info current batch\n",
    "    if os.path.isfile(header_path):\n",
    "        with open(header_path) as infile:\n",
    "            header = infile.read().split(',')\n",
    "        for f in f_paths_run_name:\n",
    "            input_dicts = read_csv(f, header = header)\n",
    "            input_dicts_run_exp.extend(input_dicts)\n",
    "            if 'TEST' not in f:\n",
    "                f_name = f.split('/')[-1]\n",
    "                batch_n = int(f_name.split('-')[2].split('.')[0][len('batch'):])\n",
    "                ##../prolific_input/run3-group_experiment1/qu70-s_qu70-batch11.csv\n",
    "                batch_numbers.append(batch_n)\n",
    "        # get all input dicts:\n",
    "        for f in f_paths_all:\n",
    "            #run = f.replace('../prolific_input/', '').split('-')[0]\n",
    "            #../prolific_input/run3-group_experiment1/qu70-s_qu70-batch11.csv\n",
    "            input_dicts = read_csv(f, header = header)\n",
    "            all_input_dicts.extend(input_dicts)\n",
    "    return all_input_dicts, input_dicts_run_exp, batch_numbers\n",
    "\n",
    "\n",
    "run = '5_pilot'\n",
    "experiment_name = 'experiment3'\n",
    "\n",
    "exp_dict = dict()\n",
    "all_input_dicts, input_dicts_exp_run, batch_numbers = read_input(run, experiment_name)\n",
    "print(len(all_input_dicts))\n",
    "print(len(input_dicts_batch))\n",
    "\n",
    "# get all questions in run\n",
    "question_path = f'../questions/run{run}-all-restricted_True.csv'\n",
    "question_dicts = read_csv(question_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(batch_numbers)\n",
    "question_path = f'../questions/run{run}-all-restricted_True.csv'\n",
    "question_dicts = read_csv(question_path)\n",
    "selected_properties = read_group(experiment_name)\n",
    "test_question_path = f'../questions/run{run}-TEST.csv'\n",
    "test_question_dicts = read_csv(test_question_path)\n",
    "\n",
    "print('available for batch:')\n",
    "questions_to_annotate_batch, invalid_annotations = get_available_questions(input_dicts_batch,\\\n",
    "                                                                    question_dicts)\n",
    "print('availabel in total:')\n",
    "questions_to_annotate_total, invalid_annotations = get_available_questions(all_input_dicts,\\\n",
    "                                                                    question_dicts)\n",
    "questions_in_selection = [d for d in questions_to_annotate_batch\n",
    "                          if d['property'] in selected_properties]\n",
    "questions_in_selection_total = [d for d in question_dicts\n",
    "                          if d['property'] in selected_properties]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
