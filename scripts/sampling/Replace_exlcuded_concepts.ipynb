{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace excluded concepts\n",
    "\n",
    "\n",
    "## Basis: stratified sampling\n",
    "\n",
    "## Steps:\n",
    "* vocabulary information has been updated with mrc norms for all concepts (also for the ones extracted from the space) :check: \n",
    "* recreated bins on updated data --> copy to the this repo :check:\n",
    "* rerun concept dataset lexical information script :check:\n",
    "* move new data to current repo - check this!\n",
    "* analyze dataset in terms of bins :check:\n",
    "* draw from remaining candidates in underrepresented bins  :check:\n",
    "* make random aspect replicable\n",
    "\n",
    "\n",
    "Move to py scripts and:\n",
    "* restructure data respository and make sure scripts align\n",
    "* write new set to files in data repository\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexical_data():\n",
    "    # this is what we originally sampled from\n",
    "    path = '../../data_lexical_info/all_lodce_mrc.csv'\n",
    "    \n",
    "    with open(path) as infile:\n",
    "        dicts = list(csv.DictReader(infile))\n",
    "    word_info_dict = defaultdict(list)\n",
    "    for d in dicts:\n",
    "        word = d['word']\n",
    "        word_info_dict[word].append(d)\n",
    "    return word_info_dict\n",
    "\n",
    "\n",
    "def get_concepts_set(p, col):\n",
    "    concept_info_dict = dict()\n",
    "\n",
    "    path = f'../../data_all_candidates/concepts_additional_info/{col}/{p}.csv'\n",
    "\n",
    "    with open(path) as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        dicts = list(reader)\n",
    "\n",
    "    for d in dicts:\n",
    "        concept = d['lemma']\n",
    "        filter_dec = d['filter']\n",
    "        if filter_dec == 'True':\n",
    "            concept_info_dict[concept] = d\n",
    "    return concept_info_dict\n",
    "\n",
    "\n",
    "\n",
    "def load_general_bins():\n",
    "    with open('../../vocabulary_data/bins_updated.json') as infile:\n",
    "        bin_dict_general = json.load(infile)\n",
    "    return bin_dict_general\n",
    "\n",
    "def load_cosine_bins_prop(set_info_dict):\n",
    "    #set_info_dict = get_concepts_set(p, col)\n",
    "    cosines = [float(d['cosine_centroid']) for c, d in set_info_dict.items()]\n",
    "    values, bin_intervals = np.histogram(cosines, bins = 3)\n",
    "    bin_dict_cos = bins_to_dict('cosine_centroid', values, bin_intervals)\n",
    "    return bin_dict_cos\n",
    " \n",
    "def bins_to_dict(name, values, bin_intervals, \n",
    "                 mapping=None, restriction=None, \n",
    "                 bin_type='distribution'):\n",
    "\n",
    "    bin_dict = dict()\n",
    "    bin_dict[name] =  {\n",
    "    'type' : bin_type,\n",
    "    'mapping' : mapping,\n",
    "    'bins' : [],\n",
    "    'frequencies' : [int(f) for f in list(values)],\n",
    "    'restriction' : restriction\n",
    "    }\n",
    "\n",
    "\n",
    "    for n, i in enumerate(bin_intervals):\n",
    "        if n != len(bin_intervals) - 1:\n",
    "            bin_dict[name]['bins'].append((i, bin_intervals[n+1]))\n",
    "        else:\n",
    "            break\n",
    "    return bin_dict\n",
    "\n",
    "\n",
    "def assign_to_bin(concept_dict, bin_dict, name):\n",
    "\n",
    "    #get_polysemy_info(concept_dict)\n",
    "\n",
    "    if name == 'polysemy':\n",
    "        concept_value = get_polysemy_info(concept_dict)\n",
    "        target_bin = concept_value\n",
    "    else:\n",
    "        if concept_dict[name] != '':\n",
    "            concept_value = float(concept_dict[name])\n",
    "            if bin_dict[name]['mapping'] == 'log':\n",
    "                concept_value = math.log(concept_value)\n",
    "            n_bins = len(bin_dict[name]['bins'])\n",
    "            for n, interval in enumerate(bin_dict[name]['bins']):\n",
    "                start, end = interval\n",
    "                if n < (n_bins-1):\n",
    "                    if start <= concept_value < end:\n",
    "                        target_bin = n\n",
    "                        break\n",
    "                    else:\n",
    "                        target_bin = None\n",
    "                else:\n",
    "                    if start <= concept_value <= end:\n",
    "                        target_bin = n\n",
    "                        break\n",
    "                    else:\n",
    "                        target_bin = None\n",
    "                    \n",
    "        else:\n",
    "            target_bin = None\n",
    "    return target_bin\n",
    "\n",
    "\n",
    "def get_polysemy_info(concept_dict):\n",
    "\n",
    "    word = concept_dict['word']\n",
    "    mipvu_met = concept_dict['mipvu']\n",
    "    polysemy_type = concept_dict['polysemy_type']\n",
    "\n",
    "    if polysemy_type == 'mon':\n",
    "        poly = 'mon'\n",
    "    elif polysemy_type == 'homonyms_also_same_pos':\n",
    "        poly = 'homonym'\n",
    "    elif mipvu_met == 'True':\n",
    "        poly = 'met'\n",
    "    # Possibly metonymy if not metaphor and not homonym\n",
    "    # caveat: the metaphor annotations are not exhaustive\n",
    "    elif polysemy_type == 'poly':\n",
    "        poly = 'poly_metonymy'\n",
    "    else:\n",
    "        poly = None\n",
    "    return poly\n",
    "\n",
    "\n",
    "def get_bin_feature_dict(general_bin_dict, concept_dicts):\n",
    "    concept_features_dict = dict()\n",
    "    for concept_dict in concept_dicts:\n",
    "        features_dict = dict()\n",
    "        concept = concept_dict['lemma']\n",
    "        for name in general_bin_dict.keys():\n",
    "            target_bin = assign_to_bin(concept_dict, general_bin_dict, name)\n",
    "            features_dict[name] = target_bin\n",
    "        features_dict['label']  = concept_dict['label']\n",
    "        concept_features_dict[concept] = features_dict\n",
    "    return concept_features_dict\n",
    "\n",
    "\n",
    "def get_ranked_bin_imbalances(general_bin_dict, set_bin_features, concepts_selected):\n",
    "    name_diff_dict = dict()\n",
    "    n_concepts = len(concepts_selected)\n",
    "    \n",
    "    bin_diff_tuples = []\n",
    "    for name in general_bin_dict:\n",
    "        bin_concept_cnt = Counter()\n",
    "        n_bins = len(general_bin_dict[name]['bins'])\n",
    "        n_equal_distribution = n_concepts/n_bins\n",
    "        for concept in concepts_selected:\n",
    "            f = set_bin_features[concept][name]\n",
    "            bin_concept_cnt[f] += 1\n",
    "        \n",
    "        for bin_name, cnt in bin_concept_cnt.items():\n",
    "            diff_to_equal = n_equal_distribution - cnt\n",
    "            diff_to_equal_percent = diff_to_equal/n_concepts\n",
    "            # only include if there are fewer concepts than expected:\n",
    "            if diff_to_equal > 0:\n",
    "                bin_diff_tuples.append((diff_to_equal_percent, name, bin_name))\n",
    "    \n",
    "    # sort from biggest to smallest:\n",
    "    sorted_diff_name_tuples = sorted(bin_diff_tuples, reverse=True)\n",
    "    return sorted_diff_name_tuples\n",
    "\n",
    "\n",
    "def find_equivalents(set_bin_features, concepts_not_selected, concept_dicts_exclude):\n",
    "    \n",
    "    replacement_concepts = set()\n",
    "    #no_replacement_found_concepts = set()\n",
    "    \n",
    "    features_not_selected = dict()\n",
    "    for concept in concepts_not_selected:\n",
    "        features_not_selected[concept] = set_bin_features[concept]\n",
    "\n",
    "    for d in concept_dicts_exclude:\n",
    "        concept = d['lemma']\n",
    "        features = set_bin_features[concept]\n",
    "        if features in features_not_selected.values():\n",
    "            for concept_available, feats_available in features_not_selected.items():\n",
    "                if features == feats_available:\n",
    "                    replacement_concepts.add(concept_available)\n",
    "                    break\n",
    "\n",
    "        #else:\n",
    "         #   no_replacement_found_concepts.add(concept)\n",
    "    return replacement_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opener dict_keys(['label', 'categories_str', 'sources_str', 'certainty', 'cosine_centroid', 'manual_coarse_grained', 'space_selection', 'qumcrae_label', 'word', 'lemma', 'wiki_frequency', 'word_in_wn?', 'word_noun_in_wn?', 'word_noun_spacy?', 'n_navigli_clusters', 'n_onto_senses_n_v', 'n_wn_senses', 'min_wn_sim_wup', 'av_sim_wup', 'polysemy_type', 'mipvu', 'wn_abs_conc', 'filter', 'conc', 'fam', 'aoa'])\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "# Create mapping between prop and collection name - load entire set\n",
    "props_collection_dict = {'used_in_cooking': 'complex', 'warm': 'perceptual', 'black': 'perceptual'}\n",
    "# used_in_cooking\n",
    "p = 'used_in_cooking'\n",
    "col = props_collection_dict[p]\n",
    "set_info_dict = get_concepts_set(p, col)\n",
    "for c, info_dict in set_info_dict.items():\n",
    "    print(c, info_dict.keys())\n",
    "    break\n",
    "print(len(set_info_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bins\n",
    "general_bin_dict = load_general_bins()\n",
    "bin_dict_cosine = load_cosine_bins_prop(set_info_dict)\n",
    "general_bin_dict.update(bin_dict_cosine)\n",
    "\n",
    "#general_bin_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gavel\n",
      "0.2067427267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_concept, test_concept_dict = list(set_info_dict.items())[10]\n",
    "print(test_concept)\n",
    "print(test_concept_dict['cosine_centroid'])\n",
    "target_bin = assign_to_bin(test_concept_dict, general_bin_dict, 'cosine_centroid')\n",
    "target_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157 22\n",
      "set()\n",
      "Concepts still available for sampling: 71\n"
     ]
    }
   ],
   "source": [
    "# load_concept_dicts include and excluse\n",
    "concept_dicts_exclude, concept_dicts_include =  get_excluded_included_concepts(p)\n",
    "print(len(concept_dicts_include), len(concept_dicts_exclude))\n",
    "\n",
    "concept_dicts_total = concept_dicts_include + concept_dicts_exclude\n",
    "concepts_selected = set([d['lemma'] for d in concept_dicts_total])\n",
    "\n",
    "total_concepts = set(set_info_dict.keys())\n",
    "concepts_not_selected = total_concepts.difference(concepts_selected)\n",
    "concept_dicts_not_selected = [d for c, d in set_info_dict.items()\\\n",
    "                              if c in concepts_not_selected]\n",
    "\n",
    "\n",
    "# sanity check:\n",
    "# should print empty set\n",
    "print(concepts_selected.intersection(concepts_not_selected))\n",
    "print(f'Concepts still available for sampling: {len(concept_dicts_not_selected)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort concepts into bins:\n",
    "\n",
    "# sort excluded concepts into bins\n",
    "# sort remaining dataset into bins\n",
    "\n",
    "# For each excluded concept, draw a new one from the same bin \n",
    "# If the same bin is empty, draw from another, smaller bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "{'puree', 'spreader', 'straightedge', 'vermicelli', 'bolo', 'pineapple'}\n",
      "185\n",
      "Still to replace:  16\n"
     ]
    }
   ],
   "source": [
    "# all bin names\n",
    "print(len(concepts_selected))\n",
    "set_bin_features = get_bin_feature_dict(general_bin_dict, set_info_dict.values())\n",
    "direct_replacements = find_equivalents(set_bin_features, concepts_not_selected, concept_dicts_exclude)\n",
    "print(direct_replacements)\n",
    "concepts_selected.update(direct_replacements)\n",
    "print(len(concepts_selected))\n",
    "n_to_replace = len(concept_dicts_exclude) - len(direct_replacements)\n",
    "print('Still to replace: ', n_to_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.3279279279279279, 'conc', 0)\n",
      "(0.3279279279279279, 'aoa', 2)\n",
      "(0.3225225225225225, 'fam', 0)\n",
      "(0.3009009009009009, 'conc', 1)\n",
      "(0.27387387387387385, 'aoa', 1)\n",
      "(0.26846846846846845, 'fam', 1)\n",
      "(0.24684684684684682, 'aoa', 0)\n",
      "(0.2445945945945946, 'label', 'pos/neg')\n",
      "(0.17972972972972973, 'polysemy', 'homonym')\n",
      "(0.13333333333333333, 'wiki_frequency', 1)\n",
      "(0.057657657657657645, 'cosine_centroid', 1)\n",
      "(0.04144144144144143, 'cosine_centroid', 0)\n",
      "(0.019819819819819808, 'fam', 2)\n",
      "(0.006756756756756757, 'polysemy', 'mon')\n"
     ]
    }
   ],
   "source": [
    "# print imablance before sampling\n",
    "# add labels info\n",
    "general_bin_dict['label'] = {'bins': ['pos', 'neg', 'pos/neg', 'neg/pos']}\n",
    "bins_sorted_original = get_ranked_bin_imbalances(general_bin_dict, set_bin_features, concepts_selected)\n",
    "\n",
    "for b in bins_sorted_original:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to balance bins by selecting a word from the one that is least balanced \n",
    "\n",
    "def resample_missing_concepts(bins_sorted_original, \n",
    "                              n_to_replace, \n",
    "                              concepts_not_selected,\n",
    "                              concepts_selected,\n",
    "                              set_bin_features):\n",
    "    replacement_concepts = set()\n",
    "    bins_sorted = bins_sorted_original\n",
    "    while len(replacement_concepts) < n_to_replace:\n",
    "        # get bin overview\n",
    "        for bin_tuple in bins_sorted:\n",
    "            if len(replacement_concepts) == n_to_replace:\n",
    "                print('found enough!')\n",
    "                break\n",
    "            name = bin_tuple[1]\n",
    "            bin_name = bin_tuple[2]\n",
    "            concepts_not_selected_shuff = list(concepts_not_selected)\n",
    "            random.shuffle(concepts_not_selected_shuff)\n",
    "            # shuffle original concept list so it's not sorted by cosine distance\n",
    "            for c in concepts_not_selected_shuff:\n",
    "                features = set_bin_features[c]\n",
    "                if len(replacement_concepts) == n_to_replace:\n",
    "                    print('found enough inside!')\n",
    "                    break\n",
    "                if features[name] == bin_name:\n",
    "                    print('replacement found in ', name, bin_name)\n",
    "                    replacement_concepts.add(c)        \n",
    "        concepts_selected.update(replacement_concepts)\n",
    "        bins_sorted = get_ranked_bin_imbalances(general_bin_dict, set_bin_features, concepts_selected)\n",
    "    return replacement_concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacement found in  fam 1\n",
      "replacement found in  aoa 0\n",
      "replacement found in  wiki_frequency 1\n",
      "replacement found in  wiki_frequency 1\n",
      "replacement found in  wiki_frequency 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "found enough inside!\n",
      "found enough!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'bean',\n",
       " 'bolo',\n",
       " 'chopper',\n",
       " 'crampon',\n",
       " 'flour',\n",
       " 'ham',\n",
       " 'hoe',\n",
       " 'plunger',\n",
       " 'scallop',\n",
       " 'scythe',\n",
       " 'sledgehammer',\n",
       " 'spinach',\n",
       " 'spoon',\n",
       " 'spreader',\n",
       " 'strainer',\n",
       " 'teaspoon'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resample_missing_concepts(bins_sorted_original, \n",
    "                              n_to_replace, \n",
    "                              concepts_not_selected,\n",
    "                              concepts_selected,\n",
    "                              set_bin_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.3284552845528455, 'conc', 0)\n",
      "(0.3284552845528455, 'aoa', 2)\n",
      "(0.3235772357723577, 'fam', 0)\n",
      "(0.30406504065040646, 'conc', 1)\n",
      "(0.27967479674796747, 'aoa', 1)\n",
      "(0.26991869918699185, 'fam', 1)\n",
      "(0.2504065040650406, 'aoa', 0)\n",
      "(0.2451219512195122, 'label', 'pos/neg')\n",
      "(0.18658536585365854, 'polysemy', 'homonym')\n",
      "(0.1382113821138211, 'wiki_frequency', 1)\n",
      "(0.060162601626016235, 'cosine_centroid', 0)\n",
      "(0.0260162601626016, 'fam', 2)\n",
      "(0.016260162601625994, 'cosine_centroid', 1)\n",
      "(0.0016260162601625784, 'conc', 2)\n",
      "(0.0012195121951219512, 'polysemy', 'mon')\n"
     ]
    }
   ],
   "source": [
    "for b in bins_sorted:\n",
    "    print(b)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.3279279279279279, 'conc', 0) (0.3284552845528455, 'conc', 0)\n",
      "(0.3279279279279279, 'aoa', 2) (0.3284552845528455, 'aoa', 2)\n",
      "(0.3225225225225225, 'fam', 0) (0.3235772357723577, 'fam', 0)\n",
      "(0.3009009009009009, 'conc', 1) (0.30406504065040646, 'conc', 1)\n",
      "(0.27387387387387385, 'aoa', 1) (0.27967479674796747, 'aoa', 1)\n",
      "(0.26846846846846845, 'fam', 1) (0.26991869918699185, 'fam', 1)\n",
      "(0.24684684684684682, 'aoa', 0) (0.2504065040650406, 'aoa', 0)\n",
      "(0.2445945945945946, 'label', 'pos/neg') (0.2451219512195122, 'label', 'pos/neg')\n",
      "(0.17972972972972973, 'polysemy', 'homonym') (0.18658536585365854, 'polysemy', 'homonym')\n",
      "(0.13333333333333333, 'wiki_frequency', 1) (0.1382113821138211, 'wiki_frequency', 1)\n",
      "(0.057657657657657645, 'cosine_centroid', 1) (0.060162601626016235, 'cosine_centroid', 0)\n",
      "(0.04144144144144143, 'cosine_centroid', 0) (0.0260162601626016, 'fam', 2)\n",
      "(0.019819819819819808, 'fam', 2) (0.016260162601625994, 'cosine_centroid', 1)\n",
      "(0.006756756756756757, 'polysemy', 'mon') (0.0016260162601625784, 'conc', 2)\n"
     ]
    }
   ],
   "source": [
    "for b1, b2 in zip(bins_sorted_original, bins_sorted):\n",
    "    print(b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flour\n",
      "scallop\n",
      "rhubarb\n",
      "ham\n",
      "edger\n",
      "strainer\n",
      "hoe\n",
      "toaster\n",
      "bolo\n",
      "bean\n",
      "spreader\n",
      "chopper\n",
      "grasshopper\n",
      "teaspoon\n",
      "razor\n",
      "sledgehammer\n",
      "crampon\n",
      "spinach\n",
      "scythe\n",
      "parer\n",
      "plunger\n",
      "spoon\n"
     ]
    }
   ],
   "source": [
    "for c in replacement_concepts:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
