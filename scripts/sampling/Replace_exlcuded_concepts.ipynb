{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replace excluded concepts\n",
    "\n",
    "\n",
    "## Basis: stratified sampling\n",
    "\n",
    "## Steps:\n",
    "* vocabulary information has been updated with mrc norms for all concepts (also for the ones extracted from the space) :check: \n",
    "* recreated bins on updated data --> copy to the this repo :check:\n",
    "* rerun concept dataset lexical information script :check:\n",
    "* move new data to current repo\n",
    "* analyze dataset in terms of bins\n",
    "* draw from remaining candidates in underrepresented bins \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexical_data():\n",
    "    # this is what we originally sampled from\n",
    "    path = '../../data_lexical_info/all_lodce_mrc.csv'\n",
    "    \n",
    "    with open(path) as infile:\n",
    "        dicts = list(csv.DictReader(infile))\n",
    "    word_info_dict = defaultdict(list)\n",
    "    for d in dicts:\n",
    "        word = d['word']\n",
    "        word_info_dict[word].append(d)\n",
    "    return word_info_dict\n",
    "\n",
    "\n",
    "def get_concepts_set(p, col):\n",
    "    concept_info_dict = dict()\n",
    "\n",
    "    path = f'../../data_all_candidates/concepts_additional_info/{col}/{p}.csv'\n",
    "\n",
    "    with open(path) as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        dicts = list(reader)\n",
    "\n",
    "    for d in dicts:\n",
    "        concept = d['lemma']\n",
    "        filter_dec = d['filter']\n",
    "        if filter_dec == 'True':\n",
    "            concept_info_dict[concept] = d\n",
    "    return concept_info_dict\n",
    "\n",
    "def get_excluded_included_concepts(p):\n",
    "    \n",
    "    path = f'../../data_pair_filtering/aggregated/experiment3/{p}.csv'\n",
    "\n",
    "    with open(path) as infile:\n",
    "        concept_dicts_total = list(csv.DictReader(infile, delimiter = '\\t'))\n",
    "    concept_dicts_exclude = [d for d in concept_dicts_total\\\n",
    "                         if d['decision'].startswith('exclude')]\n",
    "    concept_dicts_include = [d for d in concept_dicts_total if d['decision'] == 'include']\n",
    "    return concept_dicts_exclude, concept_dicts_include\n",
    "\n",
    "\n",
    "def load_general_bins():\n",
    "    with open('../../vocabulary_data/bins_updated.json') as infile:\n",
    "        bin_dict_general = json.load(infile)\n",
    "    return bin_dict_general\n",
    "\n",
    "def load_cosine_bins_prop(set_info_dict):\n",
    "    #set_info_dict = get_concepts_set(p, col)\n",
    "    cosines = [float(d['cosine_centroid']) for c, d in set_info_dict.items()]\n",
    "    values, bin_intervals = np.histogram(cosines, bins = 3)\n",
    "    bin_dict_cos = bins_to_dict('cosine_centroid', values, bin_intervals)\n",
    "    return bin_dict_cos\n",
    " \n",
    "def bins_to_dict(name, values, bin_intervals, \n",
    "                 mapping=None, restriction=None, \n",
    "                 bin_type='distribution'):\n",
    "\n",
    "    bin_dict = dict()\n",
    "    bin_dict[name] =  {\n",
    "    'type' : bin_type,\n",
    "    'mapping' : mapping,\n",
    "    'bins' : [],\n",
    "    'frequencies' : [int(f) for f in list(values)],\n",
    "    'restriction' : restriction\n",
    "    }\n",
    "\n",
    "\n",
    "    for n, i in enumerate(bin_intervals):\n",
    "        if n != len(bin_intervals) - 1:\n",
    "            bin_dict[name]['bins'].append((i, bin_intervals[n+1]))\n",
    "        else:\n",
    "            break\n",
    "    return bin_dict\n",
    "\n",
    "\n",
    "def assign_to_bin(concept_dict, bin_dict, name):\n",
    "\n",
    "    #get_polysemy_info(concept_dict)\n",
    "\n",
    "    if name == 'polysemy':\n",
    "        concept_value = get_polysemy_info(concept_dict)\n",
    "        target_bin = concept_value\n",
    "    else:\n",
    "        if concept_dict[name] != '':\n",
    "            concept_value = float(concept_dict[name])\n",
    "            if bin_dict[name]['mapping'] == 'log':\n",
    "                concept_value = math.log(concept_value)\n",
    "            for n, interval in enumerate(bin_dict[name]['bins']):\n",
    "                start, end = interval\n",
    "                if start <= concept_value <= end:\n",
    "                    target_bin = n\n",
    "                    break\n",
    "                else:\n",
    "                    target_bin = None\n",
    "        else:\n",
    "            target_bin = None\n",
    "    return target_bin\n",
    "\n",
    "\n",
    "def get_polysemy_info(concept_dict):\n",
    "\n",
    "    word = concept_dict['word']\n",
    "    mipvu_met = concept_dict['mipvu']\n",
    "    polysemy_type = concept_dict['polysemy_type']\n",
    "\n",
    "    if polysemy_type == 'mon':\n",
    "        poly = 'mon'\n",
    "    elif polysemy_type == 'homonyms_also_same_pos':\n",
    "        poly = 'homonym'\n",
    "    elif mipvu_met == 'True':\n",
    "        poly = 'met'\n",
    "    # Possibly metonymy if not metaphor and not homonym\n",
    "    # caveat: the metaphor annotations are not exhaustive\n",
    "    elif polysemy_type == 'poly':\n",
    "        poly = 'poly_metonymy'\n",
    "    else:\n",
    "        poly = None\n",
    "    return poly\n",
    "\n",
    "\n",
    "def get_bin_feature_dict(general_bin_dict, concept_dicts):\n",
    "    concept_features_dict = dict()\n",
    "    for concept_dict in concept_dicts:\n",
    "        features_dict = dict()\n",
    "        concept = concept_dict['lemma']\n",
    "        for name in general_bin_dict.keys():\n",
    "            target_bin = assign_to_bin(concept_dict, general_bin_dict, name)\n",
    "            features_dict[name] = target_bin\n",
    "        features_dict['label']  = concept_dict['label']\n",
    "        concept_features_dict[concept] = features_dict\n",
    "    return concept_features_dict\n",
    "\n",
    "\n",
    "def get_ranked_bin_imbalances(general_bin_dict, set_bin_features, concepts_selected):\n",
    "    name_diff_dict = dict()\n",
    "    n_concepts = len(concepts_selected)\n",
    "    \n",
    "    bin_diff_tuples = []\n",
    "    for name in general_bin_dict:\n",
    "        bin_concept_cnt = Counter()\n",
    "        n_bins = len(general_bin_dict[name]['bins'])\n",
    "        n_equal_distribution = n_concepts/n_bins\n",
    "        for concept in concepts_selected:\n",
    "            f = set_bin_features[concept][name]\n",
    "            bin_concept_cnt[f] += 1\n",
    "        \n",
    "        for bin_name, cnt in bin_concept_cnt.items():\n",
    "            diff_to_equal = n_equal_distribution - cnt\n",
    "            diff_to_equal_percent = diff_to_equal/n_concepts\n",
    "            # only include if there are fewer concepts than expected:\n",
    "            if diff_to_equal > 0:\n",
    "                bin_diff_tuples.append((diff_to_equal_percent, name, bin_name))\n",
    "    \n",
    "    # sort from biggest to smallest:\n",
    "    sorted_diff_name_tuples = sorted(bin_diff_tuples, reverse=True)\n",
    "    return sorted_diff_name_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opener dict_keys(['label', 'categories_str', 'sources_str', 'certainty', 'cosine_centroid', 'manual_coarse_grained', 'space_selection', 'qumcrae_label', 'word', 'lemma', 'wiki_frequency', 'word_in_wn?', 'word_noun_in_wn?', 'word_noun_spacy?', 'n_navigli_clusters', 'n_onto_senses_n_v', 'n_wn_senses', 'min_wn_sim_wup', 'av_sim_wup', 'polysemy_type', 'mipvu', 'wn_abs_conc', 'filter', 'conc', 'fam', 'aoa'])\n",
      "250\n"
     ]
    }
   ],
   "source": [
    "# Create mapping between prop and collection name - load entire set\n",
    "props_collection_dict = {'used_in_cooking': 'complex', 'warm': 'perceptual', 'black': 'perceptual'}\n",
    "# used_in_cooking\n",
    "p = 'used_in_cooking'\n",
    "col = props_collection_dict[p]\n",
    "set_info_dict = get_concepts_set(p, col)\n",
    "for c, info_dict in set_info_dict.items():\n",
    "    print(c, info_dict.keys())\n",
    "    break\n",
    "print(len(set_info_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bins\n",
    "general_bin_dict = load_general_bins()\n",
    "bin_dict_cosine = load_cosine_bins_prop(set_info_dict)\n",
    "general_bin_dict.update(bin_dict_cosine)\n",
    "\n",
    "#general_bin_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gavel\n",
      "0.2067427267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_concept, test_concept_dict = list(set_info_dict.items())[10]\n",
    "print(test_concept)\n",
    "print(test_concept_dict['cosine_centroid'])\n",
    "target_bin = assign_to_bin(test_concept_dict, general_bin_dict, 'cosine_centroid')\n",
    "target_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157 22\n",
      "set()\n",
      "Concepts still available for sampling: 71\n"
     ]
    }
   ],
   "source": [
    "# load_concept_dicts include and excluse\n",
    "\n",
    "concept_dicts_exclude, concept_dicts_include =  get_excluded_included_concepts(p)\n",
    "print(len(concept_dicts_include), len(concept_dicts_exclude))\n",
    "\n",
    "concept_dicts_total = concept_dicts_include + concept_dicts_exclude\n",
    "concepts_selected = set([d['lemma'] for d in concept_dicts_total])\n",
    "\n",
    "total_concepts = set(set_info_dict.keys())\n",
    "concepts_not_selected = total_concepts.difference(concepts_selected)\n",
    "concept_dicts_not_selected = [d for c, d in set_info_dict.items()\\\n",
    "                              if c in concepts_not_selected]\n",
    "\n",
    "# sanity check:\n",
    "# should print empty set\n",
    "print(concepts_selected.intersection(concepts_not_selected))\n",
    "print(f'Concepts still available for sampling: {len(concept_dicts_not_selected)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort concepts into bins:\n",
    "\n",
    "# sort excluded concepts into bins\n",
    "# sort remaining dataset into bins\n",
    "\n",
    "# For each excluded concept, draw a new one from the same bin \n",
    "# If the same bin is empty, draw from another, smaller bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bolo', 'spreader', 'tofu', 'straightedge', 'pineapple', 'kebab'}\n",
      "\n",
      "{'style', 'jack', 'starter', 'dish', 'adze', 'salad', 'tap', 'tool', 'bit'}\n"
     ]
    }
   ],
   "source": [
    "# all bin names\n",
    "\n",
    "\n",
    "set_bin_features = get_bin_feature_dict(general_bin_dict, set_info_dict.values())\n",
    "\n",
    "features_not_selected = dict()\n",
    "for concept in concepts_not_selected:\n",
    "    features_not_selected[concept] = set_bin_features[concept]\n",
    "    \n",
    "    \n",
    "replacement_concepts = set()\n",
    "no_replacement_found_concepts = set()\n",
    "\n",
    "for d in concept_dicts_exclude:\n",
    "    concept = d['lemma']\n",
    "    features = set_bin_features[concept]\n",
    "    if features in features_not_selected.values():\n",
    "        for concept_available, feats_available in features_not_selected.items():\n",
    "            if features == feats_available:\n",
    "                replacement_concepts.add(concept_available)\n",
    "                break\n",
    "        \n",
    "    else:\n",
    "        no_replacement_found_concepts.add(concept)\n",
    "\n",
    "print(replacement_concepts)\n",
    "print()\n",
    "print(no_replacement_found_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.32774674115456237, 'conc', 0)\n",
      "(0.32774674115456237, 'aoa', 2)\n",
      "(0.3221601489757914, 'fam', 0)\n",
      "(0.2998137802607076, 'conc', 1)\n",
      "(0.2718808193668529, 'aoa', 1)\n",
      "(0.26629422718808193, 'fam', 1)\n",
      "(0.24441340782122906, 'label', 'pos/neg')\n",
      "(0.24394785847299813, 'aoa', 0)\n",
      "(0.17737430167597765, 'polysemy', 'homonym')\n",
      "(0.12662942271880817, 'wiki_frequency', 1)\n",
      "(0.05959031657355678, 'cosine_centroid', 1)\n",
      "(0.03724394785847299, 'cosine_centroid', 0)\n",
      "(0.014897579143389185, 'fam', 2)\n",
      "(0.009776536312849162, 'polysemy', 'mon')\n"
     ]
    }
   ],
   "source": [
    "# print imablance before sampling\n",
    "# add labels info\n",
    "general_bin_dict['label'] = {'bins': ['pos', 'neg', 'pos/neg', 'neg/pos']}\n",
    "bins_sorted_original = get_ranked_bin_imbalances(general_bin_dict, set_bin_features, concepts_selected)\n",
    "\n",
    "for b in bins_sorted_original:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacement found in  fam 1\n",
      "replacement found in  aoa 0\n",
      "replacement found in  wiki_frequency 1\n",
      "replacement found in  wiki_frequency 1\n",
      "replacement found in  wiki_frequency 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 1\n",
      "replacement found in  cosine_centroid 0\n",
      "replacement found in  cosine_centroid 0\n",
      "found enough inside!\n",
      "found enough!\n"
     ]
    }
   ],
   "source": [
    "# try to balance bins by selecting a word from the one that is least balanced \n",
    "\n",
    "replacement_concepts = set()\n",
    "bins_sorted = bins_sorted_original\n",
    "while len(replacement_concepts) < len(no_replacement_found_concepts):\n",
    "    # get bin overview\n",
    "    for bin_tuple in bins_sorted:\n",
    "        if len(replacement_concepts) == len(concept_dicts_exclude):\n",
    "            print('found enough!')\n",
    "            break\n",
    "        name = bin_tuple[1]\n",
    "        bin_name = bin_tuple[2]\n",
    "        concepts_not_selected_shuff = list(concepts_not_selected)\n",
    "        random.shuffle(concepts_not_selected_shuff)\n",
    "        # shuffle original concept list so it's not sorted by cosine distance\n",
    "        for c in concepts_not_selected_shuff:\n",
    "            features = set_bin_features[c]\n",
    "            if len(replacement_concepts) == len(concept_dicts_exclude):\n",
    "                print('found enough inside!')\n",
    "                break\n",
    "            if features[name] == bin_name:\n",
    "                print('replacement found in ', name, bin_name)\n",
    "                replacement_concepts.add(c)        \n",
    "    concepts_selected.update(replacement_concepts)\n",
    "    bins_sorted = get_ranked_bin_imbalances(general_bin_dict, set_bin_features, concepts_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "print(len(replacement_concepts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.32838283828382836, 'conc', 0)\n",
      "(0.32838283828382836, 'aoa', 2)\n",
      "(0.3234323432343234, 'fam', 0)\n",
      "(0.3036303630363036, 'conc', 1)\n",
      "(0.27887788778877887, 'aoa', 1)\n",
      "(0.26897689768976896, 'fam', 1)\n",
      "(0.24917491749174916, 'aoa', 0)\n",
      "(0.24504950495049505, 'label', 'pos/neg')\n",
      "(0.18564356435643564, 'polysemy', 'homonym')\n",
      "(0.13531353135313529, 'wiki_frequency', 1)\n",
      "(0.056105610561056084, 'cosine_centroid', 0)\n",
      "(0.021452145214521427, 'fam', 2)\n",
      "(0.011551155115511528, 'cosine_centroid', 1)\n",
      "(0.007425742574257425, 'polysemy', 'mon')\n"
     ]
    }
   ],
   "source": [
    "for b in bins_sorted:\n",
    "    print(b)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.32774674115456237, 'conc', 0) (0.32838283828382836, 'conc', 0)\n",
      "(0.32774674115456237, 'aoa', 2) (0.32838283828382836, 'aoa', 2)\n",
      "(0.3221601489757914, 'fam', 0) (0.3234323432343234, 'fam', 0)\n",
      "(0.2998137802607076, 'conc', 1) (0.3036303630363036, 'conc', 1)\n",
      "(0.2718808193668529, 'aoa', 1) (0.27887788778877887, 'aoa', 1)\n",
      "(0.26629422718808193, 'fam', 1) (0.26897689768976896, 'fam', 1)\n",
      "(0.24441340782122906, 'label', 'pos/neg') (0.24917491749174916, 'aoa', 0)\n",
      "(0.24394785847299813, 'aoa', 0) (0.24504950495049505, 'label', 'pos/neg')\n",
      "(0.17737430167597765, 'polysemy', 'homonym') (0.18564356435643564, 'polysemy', 'homonym')\n",
      "(0.12662942271880817, 'wiki_frequency', 1) (0.13531353135313529, 'wiki_frequency', 1)\n",
      "(0.05959031657355678, 'cosine_centroid', 1) (0.056105610561056084, 'cosine_centroid', 0)\n",
      "(0.03724394785847299, 'cosine_centroid', 0) (0.021452145214521427, 'fam', 2)\n",
      "(0.014897579143389185, 'fam', 2) (0.011551155115511528, 'cosine_centroid', 1)\n",
      "(0.009776536312849162, 'polysemy', 'mon') (0.007425742574257425, 'polysemy', 'mon')\n"
     ]
    }
   ],
   "source": [
    "for b1, b2 in zip(bins_sorted_original, bins_sorted):\n",
    "    print(b1, b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
