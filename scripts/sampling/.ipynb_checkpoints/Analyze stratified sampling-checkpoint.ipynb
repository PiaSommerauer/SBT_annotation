{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze stratified sampling\n",
    "\n",
    "\n",
    "## Aspects considered\n",
    "\n",
    "* wikipedia frequency\n",
    "* Concreteness (mrc)\n",
    "* familiarity (mrc)\n",
    "* age of acquisition (mrc)\n",
    "* polysemy\n",
    "* similarity to centroid --> in sampling script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original bin function (use it for cosine similarity to centroid)\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import random\n",
    "\n",
    "def bins_to_dict(name, frequencies, bin_intervals, mapping, restriction, bin_type):\n",
    "\n",
    "    bin_dict = dict()\n",
    "    bin_dict[name] =  {\n",
    "    'type' : bin_type,\n",
    "    'mapping' : mapping,\n",
    "    'bins' : [],\n",
    "    'frequencies' : [int(f) for f in list(frequencies)],\n",
    "    'restriction' : restriction\n",
    "    }\n",
    "\n",
    "\n",
    "    for n, i in enumerate(bin_intervals):\n",
    "        if n != len(bin_intervals) - 1:\n",
    "            bin_dict[name]['bins'].append((i, bin_intervals[n+1]))\n",
    "        else:\n",
    "            break\n",
    "    return bin_dict\n",
    "\n",
    "def get_bins_from_distribution(concept_dict_list, name, n_bins, \\\n",
    "                            mapping = False, restriction = None):\n",
    "\n",
    "    if mapping == False:\n",
    "        if restriction == None:\n",
    "            values = [float(d[name]) for d in concept_dict_list if d[name] != '']\n",
    "        else:\n",
    "            print('restricting data')\n",
    "            values = [float(d[name]) for d in concept_dict_list if \\\n",
    "                    (d[name] != '') and (float(d[name]) != restriction)]\n",
    "    elif mapping == 'log':\n",
    "        print('taking the log')\n",
    "        if restriction == None:\n",
    "            values = [math.log(float(d[name])) for d in concept_dict_list if d[name] != '']\n",
    "        else:\n",
    "            print('taking log and restricting data')\n",
    "            values = [math.log(float(d[name])) for d in concept_dict_list if \\\n",
    "                    (d[feature] != '') and (float(d[name]) != restriction)]\n",
    "\n",
    "    frequencies, bin_intervals = np.histogram(values, bins = n_bins)\n",
    "    print(bin_intervals)\n",
    "    print(frequencies)\n",
    "    bin_dict = bins_to_dict(name, frequencies, bin_intervals, mapping,\\\n",
    "                            restriction, 'distribution')\n",
    "    #plt.hist(values, bins = n_bins)\n",
    "    #plt.gca().set(title='Frequency Histogram', ylabel='frequency', xlabel=feature)\n",
    "    #plt.show()\n",
    "    return bin_dict\n",
    "\n",
    "\n",
    "def assign_to_bin(concept_dict, bin_dict, name):\n",
    "\n",
    "    #get_polysemy_info(concept_dict)\n",
    "\n",
    "    if name == 'polysemy':\n",
    "        concept_value = get_polysemy_info(concept_dict)\n",
    "        bin_assigned = concept_value\n",
    "    else:\n",
    "        if concept_dict[name] != '':\n",
    "            concept_value = float(concept_dict[name])\n",
    "            if bin_dict[name]['mapping'] == 'log':\n",
    "                concept_value = math.log(concept_value)\n",
    "            for n, interval in enumerate(bin_dict[name]['bins']):\n",
    "                start, end = interval\n",
    "                if start <= concept_value < end:\n",
    "                    bin_assigned = n\n",
    "                    break\n",
    "                else:\n",
    "                    bin_assigned = None\n",
    "        else:\n",
    "            bin_assigned = None\n",
    "    return bin_assigned\n",
    "\n",
    "\n",
    "def get_polysemy_info(concept_dict):\n",
    "\n",
    "    word = concept_dict['word']\n",
    "    mipvu_met = concept_dict['mipvu']\n",
    "    polysemy_type = concept_dict['polysemy_type']\n",
    "\n",
    "    if polysemy_type == 'mon':\n",
    "        poly = 'mon'\n",
    "    elif polysemy_type == 'homonyms_also_same_pos':\n",
    "        poly = 'homonym'\n",
    "    elif mipvu_met == 'True':\n",
    "        poly = 'met'\n",
    "    # Possibly metonymy if not metaphor and not homonym\n",
    "    # caveat: the metaphor annotations are not exhaustive\n",
    "    elif polysemy_type == 'poly':\n",
    "        poly = 'poly_metonymy'\n",
    "    else:\n",
    "        poly = None\n",
    "    return poly\n",
    "\n",
    "def get_bin_distribution(bin_dict, concept_dicts, concept_info_dict, sampling_name):\n",
    "    bin_concept_dict = defaultdict(list)\n",
    "    #if sampling_name == 'polysemy':\n",
    "     #   sampling_name = sampling_name\n",
    "\n",
    "    for d in concept_dicts:\n",
    "        concept = d['lemma']\n",
    "        info = concept_info_dict[concept]\n",
    "        bin_assigned = assign_to_bin(info, bin_dict, sampling_name)\n",
    "        bin_concept_dict[bin_assigned].append(concept)\n",
    "\n",
    "    return bin_concept_dict\n",
    "\n",
    "\n",
    "def draw_random_sample_from_list(concept_dict_list):\n",
    "\n",
    "    # chose a random integer (lenth is out of list index)\n",
    "    selected_int = random.randint(0, len(concept_dict_list)-1)\n",
    "    # get data item from list and remove selected item from list\n",
    "    selected_item = concept_dict_list.pop(selected_int)\n",
    "\n",
    "    return selected_item, selected_int\n",
    "\n",
    "\n",
    "def load_lexical_data():\n",
    "    # this is what we originally sampled from\n",
    "    path = '../../vocabulary_data/all_lodce_mrc.csv'\n",
    "    \n",
    "    with open(path) as infile:\n",
    "        dicts = list(csv.DictReader(infile))\n",
    "    word_info_dict = defaultdict(list)\n",
    "    for d in dicts:\n",
    "        word = d['word']\n",
    "        word_info_dict[word].append(d)\n",
    "    return word_info_dict\n",
    "\n",
    "def get_concepts_set(p, coll):\n",
    "    concept_info_dict = dict()\n",
    "\n",
    "    p = 'used_in_cooking'\n",
    "    col = props_collection_dict[p]\n",
    "    path = f'../../data_lexical_info/concepts_additional_info/{col}/{p}.csv'\n",
    "\n",
    "    with open(path) as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        dicts = list(reader)\n",
    "\n",
    "    for d in dicts:\n",
    "        concept = d['lemma']\n",
    "        concept_info_dict[concept] = d\n",
    "    return concept_info_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_frequency {'type': 'distribution', 'mapping': 'log', 'bins': [[4.605170185988092, 9.300147501609214], [9.300147501609214, 13.995124817230336], [13.995124817230336, 18.690102132851457]], 'frequencies': [18678, 6823, 111], 'restriction': None}\n",
      "conc {'type': 'distribution', 'mapping': False, 'bins': [[158.0, 328.66666666666663], [328.66666666666663, 499.3333333333333], [499.3333333333333, 670.0]], 'frequencies': [771, 1476, 1641], 'restriction': None}\n",
      "fam {'type': 'distribution', 'mapping': False, 'bins': [[74.0, 268.33333333333337], [268.33333333333337, 462.6666666666667], [462.6666666666667, 657.0]], 'frequencies': [177, 1321, 2724], 'restriction': None}\n",
      "aoa {'type': 'distribution', 'mapping': False, 'bins': [[125.0, 315.66666666666663], [315.66666666666663, 506.3333333333333], [506.3333333333333, 697.0]], 'frequencies': [416, 964, 488], 'restriction': None}\n",
      "polysemy {'type': 'categories', 'mapping': None, 'bins': ['mon', 'met', 'poly_metonymy', 'homonym'], 'frequencies': [21844, 2339, 8991, 571], 'restriction': None}\n"
     ]
    }
   ],
   "source": [
    "# load general bins\n",
    "\n",
    "with open('../vocabulary_data/bins.json') as infile:\n",
    "    bin_dict_general = json.load(infile)\n",
    "    \n",
    "for k, v in bin_dict_general.items():\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cosine bins of a property\n",
    "import csv\n",
    "\n",
    "props_collection_dict = {'used_in_cooking': 'complex', 'warm': 'perceptual'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spade', 'cake', 'gravy', 'jack', 'sauce', 'pestle', 'cleaver', 'canary', 'file', 'tablespoon', 'hammer', 'dry', 'beef', 'tool', 'gavel', 'spatula', 'cabbage', 'shrimp', 'mustard', 'radish', 'knife', 'butter', 'garlic', 'lobster', 'slice', 'soup', 'bake', 'tap', 'carrot', 'plane', 'tomato', 'rice', 'spinach', 'bean', 'screwdriver', 'upset', 'pineapple', 'pie', 'see', 'corn', 'bill', 'cauliflower', 'cream', 'violin', 'eat', 'mallet', 'toaster', 'style', 'dough', 'axe', 'die', 'hatchet', 'buffer', 'square', 'pudding', 'lettuce', 'crank', 'pork', 'grasshopper', 'cook', 'round', 'lime', 'smooth', 'candy', 'cucumber', 'corkscrew', 'gang', 'spoon', 'salad', 'leek', 'ram', 'gutter', 'shovel', 'potato', 'mussel', 'cheese', 'awl', 'chisel', 'bass', 'flannel', 'bread', 'sparrow', 'punch', 'chicken', 'float', 'fork', 'noodle', 'beetle', 'rake', 'oven', 'pea', 'chop', 'vinegar', 'ham', 'hoe', 'apron', 'adze', 'mince', 'steak', 'bit', 'stew', 'pick', 'shear', 'meat', 'stove', 'pickle', 'broil', 'blade', 'mash', 'onion', 'vegetable', 'drill', 'asparagus', 'razor'}\n",
      "\n",
      "{'plier', 'pandemis', 'stamp', 'mower', 'tteok', 'pilocrocis', 'artichoke', 'grappler', 'allspice', 'jackknife', 'brioche', 'refried', 'focaccia', 'doenjang', 'burin', 'whisk', 'broccoli', 'clawhammer', 'blender', 'marinara', 'empanada', 'flatbread', 'sisurcana', 'lefse', 'stovetop', 'breadcrumb', 'bulgur', 'buttered', 'dessert', 'pounder', 'buttercream', 'pasta', 'slicer', 'sauteed', 'pickaxe', 'peanut', 'spreader', 'pimento', 'pizzeria', 'ax', 'borer', 'scalpel', 'pallet', 'trowel', 'calathus', 'caramelize', 'hexachaeta', 'confit', 'brown', 'slick', 'achiote', 'sapodilla', 'scoop', 'stewed', 'stylus', 'cucurbit', 'croquette', 'cutter', 'scissor', 'mattock', 'pycnarmon', 'tortilla', 'endoxyla', 'celery', 'puree', 'pepperoni', 'chorizo', 'marinade', 'fritter', 'jackhammer', 'tamper', 'omelette', 'lentil', 'enchiladas', 'homoeosoma', 'graver', 'pitchfork', 'scythe', 'topping', 'cutlet', 'sundae', 'jalapeos', 'gulai', 'turnip', 'plough', 'braise', 'bevel', 'soursop', 'acaridae', 'sativum', 'eggplant', 'burr', 'coconut', 'cilantro', 'puncher', 'trichilia', 'taco', 'scallop', 'shallot', 'shaver', 'hack', 'hypotia', 'shiitake', 'provolone', 'bur', 'jigsaw', 'grapple', 'edger', 'toothpick', 'acrolepiidae', 'sashimi', 'transtillaspis', 'flour', 'reamer', 'snip', 'bagoong', 'seasoning', 'bender', 'saute', 'stamper', 'lister', 'snack', 'spud', 'aubergine', 'casserole', 'hob', 'dibble', 'sledgehammer', 'scribe', 'lontong', 'teaspoon', 'scuffle', 'blepharomastix', 'oregano', 'wrench', 'evergestis', 'clipper', 'tofu', 'sawmill', 'ripsaw', 'risotto', 'rasp', 'sledge', 'grater', 'chive', 'fava', 'rhubarb', 'colocasia', 'pumpkin', 'leucaena', 'sander', 'parer', 'bodkin', 'sausage', 'bacon', 'planer', 'trombidium', 'barbecue', 'straightedge', 'parang', 'roasted', 'starter', 'spanner', 'pincer', 'chickpea', 'quesadilla', 'lance', 'euzophera', 'moong', 'chainsaw', 'pigweed', 'mushroom', 'atole', 'vermicelli', 'sickle', 'polisher', 'gouge', 'switchblade', 'adz', 'marlinspike', 'pastry', 'chutney', 'opener', 'razorblade', 'spiciness', 'baguette', 'congee', 'slaw', 'macchiato', 'lancet', 'penknife', 'almond', 'plow', 'chhena', 'rammer', 'fry', 'phostria', 'galangal', 'parsley', 'lygropia', 'muller', 'marinate', 'auger', 'paneer', 'payasam', 'vinaigrette', 'orthaga', 'knicker', 'mooncake', 'hacksaw', 'crpe', 'patty', 'comb', 'pancake', 'tempeh', 'maul', 'cutlery', 'mayonnaise', 'scraper', 'microwave', 'plunger', 'spathulate', 'lemongrass', 'gochujang', 'cornmeal', 'savoury', 'omiode', 'router', 'strainer', 'heckle', 'tong', 'hayfork', 'gimlet', 'parboil', 'dish', 'barong', 'paratha', 'kebab', 'prawn', 'crampon', 'condiment', 'bolo', 'clincher', 'chopper', 'burrito', 'drive'}\n"
     ]
    }
   ],
   "source": [
    "# check if words we do not have ratings for are 'weird'\n",
    "\n",
    "\n",
    "# used_in_cooking\n",
    "p = 'used_in_cooking'\n",
    "coll = props_collection_dict[p]\n",
    "set_info_dict = get_concepts_set(p, coll)\n",
    "lexical_data_dict = load_lexical_data()\n",
    "\n",
    "words_with_rating = set()\n",
    "words_without_rating = set()\n",
    "\n",
    "for w, info in set_info_dict.items():\n",
    "    lexical_info = lexical_data_dict[w]\n",
    "    fam = lexical_info[0]['fam']\n",
    "    if fam != '':\n",
    "        words_with_rating.add(w)\n",
    "    else:\n",
    "        words_without_rating.add(w)\n",
    "        \n",
    "print(words_with_rating)\n",
    "print()\n",
    "print(words_without_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spade', 'cake', 'gravy', 'jack', 'sauce', 'pestle', 'cleaver', 'canary', 'file', 'tablespoon', 'hammer', 'dry', 'beef', 'tool', 'gavel', 'spatula', 'cabbage', 'shrimp', 'mustard', 'radish', 'knife', 'butter', 'garlic', 'lobster', 'slice', 'soup', 'bake', 'tap', 'carrot', 'plane', 'tomato', 'rice', 'spinach', 'bean', 'screwdriver', 'upset', 'pineapple', 'pie', 'see', 'corn', 'bill', 'cauliflower', 'cream', 'violin', 'eat', 'mallet', 'toaster', 'style', 'dough', 'axe', 'die', 'hatchet', 'buffer', 'square', 'pudding', 'lettuce', 'crank', 'pork', 'grasshopper', 'cook', 'round', 'lime', 'smooth', 'candy', 'cucumber', 'corkscrew', 'gang', 'spoon', 'salad', 'leek', 'ram', 'gutter', 'shovel', 'potato', 'mussel', 'cheese', 'awl', 'chisel', 'bass', 'flannel', 'bread', 'sparrow', 'punch', 'chicken', 'float', 'fork', 'noodle', 'beetle', 'rake', 'oven', 'pea', 'chop', 'vinegar', 'ham', 'hoe', 'apron', 'adze', 'mince', 'steak', 'bit', 'stew', 'pick', 'shear', 'meat', 'stove', 'pickle', 'broil', 'blade', 'mash', 'onion', 'vegetable', 'drill', 'asparagus', 'razor'}\n",
      "\n",
      "{'plier', 'pandemis', 'stamp', 'mower', 'tteok', 'pilocrocis', 'artichoke', 'grappler', 'allspice', 'jackknife', 'brioche', 'refried', 'focaccia', 'doenjang', 'burin', 'whisk', 'broccoli', 'clawhammer', 'blender', 'marinara', 'empanada', 'flatbread', 'sisurcana', 'lefse', 'stovetop', 'breadcrumb', 'bulgur', 'buttered', 'dessert', 'pounder', 'buttercream', 'pasta', 'slicer', 'sauteed', 'pickaxe', 'peanut', 'spreader', 'pimento', 'pizzeria', 'ax', 'borer', 'scalpel', 'pallet', 'trowel', 'calathus', 'caramelize', 'hexachaeta', 'confit', 'brown', 'slick', 'achiote', 'sapodilla', 'scoop', 'stewed', 'stylus', 'cucurbit', 'croquette', 'cutter', 'scissor', 'mattock', 'pycnarmon', 'tortilla', 'endoxyla', 'celery', 'puree', 'pepperoni', 'chorizo', 'marinade', 'fritter', 'jackhammer', 'tamper', 'omelette', 'lentil', 'enchiladas', 'homoeosoma', 'graver', 'pitchfork', 'scythe', 'topping', 'cutlet', 'sundae', 'jalapeos', 'gulai', 'turnip', 'plough', 'braise', 'bevel', 'soursop', 'acaridae', 'sativum', 'eggplant', 'burr', 'coconut', 'cilantro', 'puncher', 'trichilia', 'taco', 'scallop', 'shallot', 'shaver', 'hack', 'hypotia', 'shiitake', 'provolone', 'bur', 'jigsaw', 'grapple', 'edger', 'toothpick', 'acrolepiidae', 'sashimi', 'transtillaspis', 'flour', 'reamer', 'snip', 'bagoong', 'seasoning', 'bender', 'saute', 'stamper', 'lister', 'snack', 'spud', 'aubergine', 'casserole', 'hob', 'dibble', 'sledgehammer', 'scribe', 'lontong', 'teaspoon', 'scuffle', 'blepharomastix', 'oregano', 'wrench', 'evergestis', 'clipper', 'tofu', 'sawmill', 'ripsaw', 'risotto', 'rasp', 'sledge', 'grater', 'chive', 'fava', 'rhubarb', 'colocasia', 'pumpkin', 'leucaena', 'sander', 'parer', 'bodkin', 'sausage', 'bacon', 'planer', 'trombidium', 'barbecue', 'straightedge', 'parang', 'roasted', 'starter', 'spanner', 'pincer', 'chickpea', 'quesadilla', 'lance', 'euzophera', 'moong', 'chainsaw', 'pigweed', 'mushroom', 'atole', 'vermicelli', 'sickle', 'polisher', 'gouge', 'switchblade', 'adz', 'marlinspike', 'pastry', 'chutney', 'opener', 'razorblade', 'spiciness', 'baguette', 'congee', 'slaw', 'macchiato', 'lancet', 'penknife', 'almond', 'plow', 'chhena', 'rammer', 'fry', 'phostria', 'galangal', 'parsley', 'lygropia', 'muller', 'marinate', 'auger', 'paneer', 'payasam', 'vinaigrette', 'orthaga', 'knicker', 'mooncake', 'hacksaw', 'crpe', 'patty', 'comb', 'pancake', 'tempeh', 'maul', 'cutlery', 'mayonnaise', 'scraper', 'microwave', 'plunger', 'spathulate', 'lemongrass', 'gochujang', 'cornmeal', 'savoury', 'omiode', 'router', 'strainer', 'heckle', 'tong', 'hayfork', 'gimlet', 'parboil', 'dish', 'barong', 'paratha', 'kebab', 'prawn', 'crampon', 'condiment', 'bolo', 'clincher', 'chopper', 'burrito', 'drive'}\n"
     ]
    }
   ],
   "source": [
    "# used_in_cooking\n",
    "p = 'warm'\n",
    "coll = props_collection_dict[p]\n",
    "set_info_dict = get_concepts_set(p, coll)\n",
    "lexical_data_dict = load_lexical_data()\n",
    "\n",
    "words_with_rating = set()\n",
    "words_without_rating = set()\n",
    "\n",
    "for w, info in set_info_dict.items():\n",
    "    lexical_info = lexical_data_dict[w]\n",
    "    fam = lexical_info[0]['fam']\n",
    "    if fam != '':\n",
    "        words_with_rating.add(w)\n",
    "    else:\n",
    "        words_without_rating.add(w)\n",
    "        \n",
    "print(words_with_rating)\n",
    "print()\n",
    "print(words_without_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "Concepts still available for sampling: 181\n"
     ]
    }
   ],
   "source": [
    "# load annotated data of a property\n",
    "\n",
    "\n",
    "\n",
    "path = f'../data_pair_filtering/aggregated/experiment3/{p}.csv'\n",
    "\n",
    "with open(path) as infile:\n",
    "    concept_dicts_total = list(csv.DictReader(infile, delimiter = '\\t'))\n",
    "\n",
    "    \n",
    "concept_dicts_include = [d for d in concept_dicts_total if d['decision'] == 'include']\n",
    "concept_dicts_exclude = [d for d in concept_dicts_total\\\n",
    "                         if d['decision'].startswith('exclude')]\n",
    "\n",
    "concepts_selected = set([d['lemma'] for d in concept_dicts_total])\n",
    "total_concepts = set(concept_info_dict.keys())\n",
    "concepts_not_selected = total_concepts.difference(concepts_selected)\n",
    "concept_dicts_not_selected = [d for c, d in concept_info_dict.items()\\\n",
    "                              if c in concepts_not_selected]\n",
    "\n",
    "# sanity check:\n",
    "# should print empty set\n",
    "print(concepts_selected.intersection(concepts_not_selected))\n",
    "print(f'Concepts still available for sampling: {len(concept_dicts_not_selected)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09478792 0.25340994 0.41203196 0.57065398]\n",
      "[ 66 103 214]\n"
     ]
    }
   ],
   "source": [
    "# get cosine similarity bins\n",
    "\n",
    "\n",
    "\n",
    "name = 'cosine_centroid'\n",
    "n_bins = 3\n",
    "bin_dict_cosine = get_bins_from_distribution(dicts, name, n_bins, \\\n",
    "                                mapping = False, restriction = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of cosine_centroid\n",
      "\n",
      "Bins:\n",
      "0 (0.0947879187, 0.2534099397666667)\n",
      "1 (0.2534099397666667, 0.4120319608333334)\n",
      "2 (0.4120319608333334, 0.5706539819)\n",
      "\n",
      "Total concepts:\n",
      "0 31\n",
      "1 67\n",
      "2 81\n",
      "\n",
      "Concepts included:\n",
      "0 26\n",
      "1 60\n",
      "2 71\n",
      "\n",
      "Concepts excluded:\n",
      "0 5\n",
      "1 7\n",
      "2 10\n"
     ]
    }
   ],
   "source": [
    "# get bin per word\n",
    "# cosine \n",
    "\n",
    "\n",
    "# inspect cosine bins\n",
    "sampling_name = 'cosine_centroid' \n",
    "print(f'Distribution of {sampling_name}')\n",
    "print()\n",
    "print('Bins:')\n",
    "for n, bin_interval in enumerate(bin_dict_cosine[sampling_name]['bins']):\n",
    "    print(n, bin_interval)\n",
    "print()\n",
    "\n",
    "\n",
    "print(f'Total concepts:')\n",
    "      \n",
    "bin_concept_dict = get_bin_distribution(bin_dict_cosine,\\\n",
    "                                        concept_dicts_total, \\\n",
    "                                        concept_info_dict,\\\n",
    "                                        sampling_name)\n",
    "       \n",
    "for b, concepts in bin_concept_dict.items():\n",
    "    print(b, len(concepts))\n",
    "    \n",
    "print()\n",
    "print(f'Concepts included:')     \n",
    "\n",
    "bin_concept_dict = get_bin_distribution(bin_dict_cosine,\\\n",
    "                                        concept_dicts_include, \\\n",
    "                                        concept_info_dict,\\\n",
    "                                        sampling_name)\n",
    "\n",
    "for b, concepts in bin_concept_dict.items():\n",
    "    print(b, len(concepts))\n",
    "    \n",
    "\n",
    "print()\n",
    "print(f'Concepts excluded:')     \n",
    "\n",
    "bin_concept_dict = get_bin_distribution(bin_dict_cosine,\\\n",
    "                                        concept_dicts_exclude, \\\n",
    "                                        concept_info_dict,\\\n",
    "                                        sampling_name)\n",
    "\n",
    "for b, concepts in bin_concept_dict.items():\n",
    "    print(b, len(concepts))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of wiki_frequency\n",
      "\n",
      "Bins:\n",
      "0 [4.605170185988092, 9.300147501609214]\n",
      "1 [9.300147501609214, 13.995124817230336]\n",
      "2 [13.995124817230336, 18.690102132851457]\n",
      "\n",
      "Total concepts:\n",
      "0 143\n",
      "1 36\n",
      "\n",
      "Concepts included:\n",
      "0 128\n",
      "1 29\n",
      "\n",
      "Concepts excluded:\n",
      "bin 1 : 7 percentage of total in the bin: 0.19\n",
      "bin 0 : 15 percentage of total in the bin: 0.1\n"
     ]
    }
   ],
   "source": [
    "# inspect fam\n",
    "sampling_name = 'wiki_frequency' \n",
    "\n",
    "print(f'Distribution of {sampling_name}')\n",
    "print()\n",
    "print('Bins:')\n",
    "for n, bin_interval in enumerate(bin_dict[sampling_name]['bins']):\n",
    "    print(n, bin_interval)\n",
    "print()\n",
    "\n",
    "\n",
    "print(f'Total concepts:')\n",
    "      \n",
    "bin_concept_dict_total = get_bin_distribution(bin_dict,\\\n",
    "                                        concept_dicts_total, \\\n",
    "                                        concept_info_dict,\\\n",
    "                                        sampling_name)\n",
    "       \n",
    "for b, concepts in bin_concept_dict_total.items():\n",
    "    print(b, len(concepts))\n",
    "    \n",
    "print()\n",
    "print(f'Concepts included:')     \n",
    "\n",
    "bin_concept_dict = get_bin_distribution(bin_dict,\\\n",
    "                                        concept_dicts_include, \\\n",
    "                                        concept_info_dict,\\\n",
    "                                        sampling_name)\n",
    "\n",
    "for b, concepts in bin_concept_dict.items():\n",
    "    print(b, len(concepts))\n",
    "    \n",
    "\n",
    "\n",
    "print()\n",
    "print(f'Concepts excluded:')     \n",
    "\n",
    "bin_concept_dict = get_bin_distribution(bin_dict,\\\n",
    "                                        concept_dicts_exclude, \\\n",
    "                                        concept_info_dict,\\\n",
    "                                        sampling_name)\n",
    "\n",
    "for b, concepts in bin_concept_dict.items():\n",
    "    print('bin', b, ':', len(concepts), 'percentage of total in the bin:', round(len(concepts)/len(bin_concept_dict_total[b]), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of fam\n",
      "\n",
      "Bins:\n",
      "0 [74.0, 268.33333333333337]\n",
      "1 [268.33333333333337, 462.6666666666667]\n",
      "2 [462.6666666666667, 657.0]\n",
      "\n",
      "Total concepts:\n",
      "2 56\n",
      "1 12\n",
      "None 109\n",
      "0 2\n",
      "\n",
      "Concepts included:\n",
      "2 50\n",
      "1 11\n",
      "None 95\n",
      "0 1\n",
      "\n",
      "Concepts excluded:\n",
      "bin 2 : 6 percentage of total in the bin: 0.11\n",
      "bin 1 : 1 percentage of total in the bin: 0.08\n",
      "bin None : 14 percentage of total in the bin: 0.13\n",
      "bin 0 : 1 percentage of total in the bin: 0.5\n"
     ]
    }
   ],
   "source": [
    "# inspect fam\n",
    "sampling_name = 'fam' \n",
    "\n",
    "print(f'Distribution of {sampling_name}')\n",
    "print()\n",
    "print('Bins:')\n",
    "for n, bin_interval in enumerate(bin_dict[sampling_name]['bins']):\n",
    "    print(n, bin_interval)\n",
    "print()\n",
    "\n",
    "\n",
    "print(f'Total concepts:')\n",
    "      \n",
    "bin_concept_dict_total = get_bin_distribution(bin_dict,\\\n",
    "                                        concept_dicts_total, \\\n",
    "                                        concept_info_dict,\\\n",
    "                                        sampling_name)\n",
    "       \n",
    "for b, concepts in bin_concept_dict_total.items():\n",
    "    print(b, len(concepts))\n",
    "    \n",
    "print()\n",
    "print(f'Concepts included:')     \n",
    "\n",
    "bin_concept_dict = get_bin_distribution(bin_dict,\\\n",
    "                                        concept_dicts_include, \\\n",
    "                                        concept_info_dict,\\\n",
    "                                        sampling_name)\n",
    "\n",
    "for b, concepts in bin_concept_dict.items():\n",
    "    print(b, len(concepts))\n",
    "    \n",
    "\n",
    "\n",
    "print()\n",
    "print(f'Concepts excluded:')     \n",
    "\n",
    "bin_concept_dict = get_bin_distribution(bin_dict,\\\n",
    "                                        concept_dicts_exclude, \\\n",
    "                                        concept_info_dict,\\\n",
    "                                        sampling_name)\n",
    "\n",
    "for b, concepts in bin_concept_dict.items():\n",
    "    print('bin', b, ':', len(concepts), 'percentage of total in the bin:', round(len(concepts)/len(bin_concept_dict_total[b]), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of aoa\n",
      "\n",
      "Bins:\n",
      "0 [125.0, 315.66666666666663]\n",
      "1 [315.66666666666663, 506.3333333333333]\n",
      "2 [506.3333333333333, 697.0]\n",
      "\n",
      "Total concepts:\n",
      "None 151\n",
      "0 16\n",
      "1 11\n",
      "2 1\n",
      "\n",
      "Concepts included:\n",
      "None 134\n",
      "0 14\n",
      "1 8\n",
      "2 1\n",
      "\n",
      "Concepts excluded:\n",
      "bin 1 : 3 percentage of total in the bin: 0.27\n",
      "bin None : 17 percentage of total in the bin: 0.11\n",
      "bin 0 : 2 percentage of total in the bin: 0.12\n"
     ]
    }
   ],
   "source": [
    "# inspect fam\n",
    "sampling_name = 'aoa' \n",
    "\n",
    "print(f'Distribution of {sampling_name}')\n",
    "print()\n",
    "print('Bins:')\n",
    "for n, bin_interval in enumerate(bin_dict[sampling_name]['bins']):\n",
    "    print(n, bin_interval)\n",
    "print()\n",
    "\n",
    "\n",
    "print(f'Total concepts:')\n",
    "      \n",
    "bin_concept_dict_total = get_bin_distribution(bin_dict,\\\n",
    "                                        concept_dicts_total, \\\n",
    "                                        concept_info_dict,\\\n",
    "                                        sampling_name)\n",
    "       \n",
    "for b, concepts in bin_concept_dict_total.items():\n",
    "    print(b, len(concepts))\n",
    "    \n",
    "print()\n",
    "print(f'Concepts included:')     \n",
    "\n",
    "bin_concept_dict = get_bin_distribution(bin_dict,\\\n",
    "                                        concept_dicts_include, \\\n",
    "                                        concept_info_dict,\\\n",
    "                                        sampling_name)\n",
    "\n",
    "for b, concepts in bin_concept_dict.items():\n",
    "    print(b, len(concepts))\n",
    "    \n",
    "\n",
    "\n",
    "print()\n",
    "print(f'Concepts excluded:')     \n",
    "\n",
    "bin_concept_dict = get_bin_distribution(bin_dict,\\\n",
    "                                        concept_dicts_exclude, \\\n",
    "                                        concept_info_dict,\\\n",
    "                                        sampling_name)\n",
    "\n",
    "for b, concepts in bin_concept_dict.items():\n",
    "    print('bin', b, ':', len(concepts), 'percentage of total in the bin:', round(len(concepts)/len(bin_concept_dict_total[b]), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of conc\n",
      "\n",
      "Bins:\n",
      "0 [158.0, 328.66666666666663]\n",
      "1 [328.66666666666663, 499.3333333333333]\n",
      "2 [499.3333333333333, 670.0]\n",
      "\n",
      "Total concepts:\n",
      "2 61\n",
      "None 111\n",
      "1 6\n",
      "0 1\n",
      "\n",
      "Concepts included:\n",
      "2 56\n",
      "None 97\n",
      "1 4\n",
      "\n",
      "Concepts excluded:\n",
      "bin 2 : 5 percentage of total in the bin: 0.08\n",
      "bin 1 : 2 percentage of total in the bin: 0.33\n",
      "bin None : 14 percentage of total in the bin: 0.13\n",
      "bin 0 : 1 percentage of total in the bin: 1.0\n"
     ]
    }
   ],
   "source": [
    "# inspect fam\n",
    "sampling_name = 'conc' \n",
    "\n",
    "print(f'Distribution of {sampling_name}')\n",
    "print()\n",
    "print('Bins:')\n",
    "for n, bin_interval in enumerate(bin_dict[sampling_name]['bins']):\n",
    "    print(n, bin_interval)\n",
    "print()\n",
    "\n",
    "\n",
    "print(f'Total concepts:')\n",
    "      \n",
    "bin_concept_dict_total = get_bin_distribution(bin_dict,\\\n",
    "                                        concept_dicts_total, \\\n",
    "                                        concept_info_dict,\\\n",
    "                                        sampling_name)\n",
    "       \n",
    "for b, concepts in bin_concept_dict_total.items():\n",
    "    print(b, len(concepts))\n",
    "    \n",
    "print()\n",
    "print(f'Concepts included:')     \n",
    "\n",
    "bin_concept_dict = get_bin_distribution(bin_dict,\\\n",
    "                                        concept_dicts_include, \\\n",
    "                                        concept_info_dict,\\\n",
    "                                        sampling_name)\n",
    "\n",
    "for b, concepts in bin_concept_dict.items():\n",
    "    print(b, len(concepts))\n",
    "    \n",
    "\n",
    "\n",
    "print()\n",
    "print(f'Concepts excluded:')     \n",
    "\n",
    "bin_concept_dict = get_bin_distribution(bin_dict,\\\n",
    "                                        concept_dicts_exclude, \\\n",
    "                                        concept_info_dict,\\\n",
    "                                        sampling_name)\n",
    "\n",
    "for b, concepts in bin_concept_dict.items():\n",
    "    print('bin', b, ':', len(concepts), 'percentage of total in the bin:', round(len(concepts)/len(bin_concept_dict_total[b]), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of polysemy\n",
      "\n",
      "Bins:\n",
      "0 mon\n",
      "1 met\n",
      "2 poly_metonymy\n",
      "3 homonym\n",
      "\n",
      "Total concepts:\n",
      "mon 43\n",
      "None 64\n",
      "poly_metonymy 59\n",
      "homonym 13\n",
      "\n",
      "Concepts included:\n",
      "mon 37\n",
      "None 54\n",
      "poly_metonymy 55\n",
      "homonym 11\n",
      "\n",
      "Concepts excluded:\n",
      "poly_metonymy 4\n",
      "homonym 2\n",
      "None 10\n",
      "mon 6\n"
     ]
    }
   ],
   "source": [
    "# inspect polysemy\n",
    "sampling_name = 'polysemy' \n",
    "\n",
    "print(f'Distribution of {sampling_name}')\n",
    "print()\n",
    "print('Bins:')\n",
    "for n, bin_interval in enumerate(bin_dict[sampling_name]['bins']):\n",
    "    print(n, bin_interval)\n",
    "print()\n",
    "\n",
    "\n",
    "print(f'Total concepts:')\n",
    "      \n",
    "bin_concept_dict = get_bin_distribution(bin_dict,\\\n",
    "                                        concept_dicts_total, \\\n",
    "                                        concept_info_dict,\\\n",
    "                                        sampling_name)\n",
    "       \n",
    "for b, concepts in bin_concept_dict.items():\n",
    "    print(b, len(concepts))\n",
    "    \n",
    "print()\n",
    "print(f'Concepts included:')     \n",
    "\n",
    "bin_concept_dict = get_bin_distribution(bin_dict,\\\n",
    "                                        concept_dicts_include, \\\n",
    "                                        concept_info_dict,\\\n",
    "                                        sampling_name)\n",
    "\n",
    "for b, concepts in bin_concept_dict.items():\n",
    "    print(b, len(concepts))\n",
    "    \n",
    "\n",
    "print()\n",
    "print(f'Concepts excluded:')     \n",
    "\n",
    "bin_concept_dict = get_bin_distribution(bin_dict,\\\n",
    "                                        concept_dicts_exclude, \\\n",
    "                                        concept_info_dict,\\\n",
    "                                        sampling_name)\n",
    "\n",
    "for b, concepts in bin_concept_dict.items():\n",
    "    print(b, len(concepts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
