{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create crowd input workflow\n",
    "\n",
    "\n",
    "1.) Create all questions of a run\n",
    "\n",
    "- this should not be changed (only created once)\n",
    "- if the question formulations change, we create a new run\n",
    "- identifiers should not change\n",
    "- create unique ids here\n",
    "- we draw from this selection for creating batches \n",
    "\n",
    "\n",
    "2.) Create batch\n",
    "\n",
    "- draw from all questions of a run\n",
    "- only select properties in the experiment group\n",
    "- always make sure that a batch contains the full set of questions of a pair \n",
    "- add prolific url \n",
    "- check prolific_input for already posed pairs\n",
    "- write to file without header\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match question ids - done\n",
    "# to clean up the data, change the ids in the questions file to the input ids - done\n",
    "\n",
    "\n",
    "# then check how much has been annotated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.listdir('../task_input/prolific_input/run3-group_experiment1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not annotated yet: 29064\n",
      "not valid 640\n",
      "questions available for annotation: 29704\n",
      "questions for current experiment: 2384\n",
      "found a new one: round 0\n",
      "found a new one: roll 10\n",
      "properties not used: 1 20\n",
      "properties not used: 1 20\n",
      "found a new one: red 20\n",
      "properties not used: 0 30\n",
      "no more properties, adding quetions: 10\n",
      "properties not used: 0 40\n",
      "no more properties, adding quetions: 10\n",
      "properties not used: 0 50\n",
      "no more properties, adding quetions: 10\n",
      "properties not used: 0 60\n",
      "no more properties, adding quetions: 10\n",
      "found enough questions 70\n",
      "created new batch with 70 questions\n",
      "added test and check, 72 questions in total.\n",
      "odict_keys(['property', 'label', 'certainty', 'concept', 'collection', 'sources', 'quid', 'relation', 'question', 'prop_pos', 'concept_pos', 'prop_neg', 'concept_neg', 'example_pos', 'example_neg'])\n"
     ]
    }
   ],
   "source": [
    "from utils import read_csv, to_csv\n",
    "from utils import sort_by_key\n",
    "from utils import read_group\n",
    "\n",
    "from random import shuffle, choice\n",
    "import os\n",
    "\n",
    "def read_input(run, experiment_name):\n",
    "    all_input_dicts = []\n",
    "    dir_path = f'../task_input/prolific_input/run{run}-group_{experiment_name}/'\n",
    "    header_path = f'{dir_path}header.txt'\n",
    "    \n",
    "    with open(header_path) as infile:\n",
    "        header = infile.read().split(',')\n",
    "    \n",
    "    filepaths = os.listdir(dir_path)\n",
    "    for f in filepaths:\n",
    "        full_path = f'{dir_path}{f}'\n",
    "        input_dicts = read_csv(full_path, header = header)\n",
    "        all_input_dicts.extend(input_dicts)\n",
    "    return all_input_dicts\n",
    "\n",
    "def collect_not_annotated(input_dicts, question_dicts):\n",
    "    \n",
    "    questions_not_annotated = []\n",
    "    input_by_quid = sort_by_key(input_dicts, ['quid'])\n",
    "    for d in question_dicts:\n",
    "        quid = d['quid']\n",
    "        if quid not in input_by_quid:\n",
    "            questions_not_annotated.append(d)\n",
    "    return questions_not_annotated\n",
    "        \n",
    "    \n",
    "def get_annotated_questions(input_dicts, question_dicts):\n",
    "    \n",
    "    input_by_quid = sort_by_key(input_dicts, ['quid'])\n",
    "    questions_by_quid = sort_by_key(question_dicts, ['quid'])\n",
    "    questions_annotated = []\n",
    "    for quid in input_by_quid:\n",
    "        if quid in questions_by_quid:\n",
    "            question = questions_by_quid[quid][0]\n",
    "            questions_annotated.append(question)\n",
    "    return questions_annotated\n",
    "    \n",
    "\n",
    "def collect_invalid(input_dicts, question_dicts):\n",
    "    \n",
    "    questions_by_pair = sort_by_key(question_dicts, ['property', 'concept'])\n",
    "    questions_annotated = get_annotated_questions(input_dicts, question_dicts)\n",
    "    questions_anntotated_by_pair = sort_by_key(questions_annotated, ['property', 'concept'])\n",
    "    invalid_annotations = []\n",
    "    \n",
    "    for pair, questions_annotated in questions_anntotated_by_pair.items():\n",
    "        questions = questions_by_pair[pair]\n",
    "        if len(questions) != len(questions_annotated):\n",
    "            #print('missing annotations for pair:', pair, len(questions), len(questions_annotated))\n",
    "            invalid_annotations.extend(questions)\n",
    "    return invalid_annotations\n",
    "\n",
    "def get_available_questions(input_dicts, question_dicts):\n",
    "    \n",
    "    questions_for_annotation = []\n",
    "    questions_not_annotated = collect_not_annotated(input_dicts, question_dicts)\n",
    "    print('not annotated yet:', len(questions_not_annotated))\n",
    "    invalid_annotations = collect_invalid(input_dicts, question_dicts)\n",
    "    print('not valid', len(invalid_annotations))\n",
    "    \n",
    "    not_annotated_pair = sort_by_key(questions_not_annotated, ['property', 'concept'])\n",
    "    invalid_pair = sort_by_key(invalid_annotations, ['property', 'concept'])\n",
    "    \n",
    "    for pair, questions in not_annotated_pair.items():\n",
    "        if pair in invalid_pair:\n",
    "            questions_for_annotation.extend(invalid_pair[pair])\n",
    "        else:\n",
    "            questions_for_annotation.extend(questions)\n",
    "            \n",
    "    wrong_n_questions = []\n",
    "    available_by_pair = sort_by_key(available_questions, ['property', 'concept'])\n",
    "    for pair, questions in available_by_pair.items():\n",
    "        if len(questions) > 10 or len(questions) < 3:\n",
    "            wrong_n_questions.append((n, pair))\n",
    "    assert len(wrong_n_questions) == 0, 'Number of questions per pair not correct.'\n",
    "    return questions_for_annotation\n",
    "    \n",
    "\n",
    "def get_check_and_test():\n",
    "    checks = read_csv('../questions/checks.csv')\n",
    "    tests = read_csv('../questions/tests.csv')\n",
    "    \n",
    "    rand_check = choice(checks)\n",
    "    rand_test = choice(tests)\n",
    "    tests_checks = [rand_check, rand_test]\n",
    "    for d in tests_checks:\n",
    "        if '' in d:\n",
    "            d.pop('')\n",
    "    return tests_checks\n",
    "\n",
    "def create_new_batch(questions_to_annotate, n_qu = 70):\n",
    "    batch = []\n",
    "    properties = set()\n",
    "    # shuffle questions:\n",
    "    shuffle(questions_to_annotate)\n",
    "    questions_by_pair = sort_by_key(questions_to_annotate, ['property', 'concept'])\n",
    "    available_properties = set([p.split('-')[0] for p in questions_by_pair.keys()])\n",
    "\n",
    "    for pair, questions in questions_by_pair.items():\n",
    "        prop = pair.split('-')[0]\n",
    "        if len(batch) < n_qu:\n",
    "            if prop not in properties:\n",
    "                print('found a new one:', prop, len(batch))\n",
    "                batch.extend(questions)\n",
    "                properties.add(prop)\n",
    "            else:\n",
    "                props_not_used = available_properties.difference(properties)\n",
    "                print('properties not used:', len(props_not_used), len(batch))\n",
    "                if len(props_not_used) > 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    batch.extend(questions)\n",
    "                    properties.add(prop)\n",
    "                    print('no more properties, adding quetions:', len(questions))\n",
    "        else:\n",
    "            print('found enough questions', len(batch))\n",
    "            break\n",
    "\n",
    "    return batch  \n",
    "\n",
    "\n",
    "def batch_to_file(batch, url, run):\n",
    "    \n",
    "    header = ['quid', 'question', 'example_pos', 'example_neg']\n",
    "    header_new = ['quid', 'description', 'exampleTrue', 'exampleFalse', 'triple', 'url']\n",
    "    new_dics = []\n",
    "    for d in batch:\n",
    "        triple = f\"{d['relation']}-{d['property']}-{d['concept']}\"\n",
    "        new_d = dict()\n",
    "        for h in header:\n",
    "            hew_d[h] = d[h]\n",
    "        new_d['triple'] = triple\n",
    "        new_d['url'] = url\n",
    "        new_dicts.append(new_d)\n",
    "    pass\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run = 3\n",
    "    experiment_name = 'experiment1'\n",
    "    input_dicts = read_input(run, experiment_name)\n",
    "    question_path = f'../questions/run{run}-all-restricted_True.csv'\n",
    "    question_dicts = read_csv(question_path)\n",
    "    selected_properties = read_group(experiment_name)\n",
    "    test_check_questions = get_check_and_test()\n",
    "\n",
    "    available_questions = get_available_questions(input_dicts, question_dicts)\n",
    "\n",
    "    questions_in_selection = [d for d in available_questions \\\n",
    "                              if d['property'] in selected_properties]\n",
    "\n",
    "\n",
    "    print('questions available for annotation:', len(questions_to_annotate))\n",
    "    print('questions for current experiment:', len(questions_in_selection))\n",
    "    new_batch = create_new_batch(questions_in_selection, n_qu = 70)\n",
    "    print(f'created new batch with {len(new_batch)} questions')\n",
    "    new_batch.extend(test_check_questions)\n",
    "    print(f'added test and check, {len(new_batch)} questions in total.')\n",
    "    print(new_batch[0].keys())\n",
    "    to_csv('test_batch.csv', new_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
